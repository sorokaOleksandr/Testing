
==> Audit <==
|---------|----------------------|----------|-----------|---------|---------------------|---------------------|
| Command |         Args         | Profile  |   User    | Version |     Start Time      |      End Time       |
|---------|----------------------|----------|-----------|---------|---------------------|---------------------|
| start   |                      | minikube | oleksandr | v1.34.0 | 06 Nov 24 14:01 EET |                     |
| stop    |                      | minikube | oleksandr | v1.34.0 | 06 Nov 24 14:05 EET |                     |
| start   |                      | minikube | oleksandr | v1.34.0 | 06 Nov 24 17:20 EET | 06 Nov 24 17:40 EET |
| service | task-message-service | minikube | oleksandr | v1.34.0 | 06 Nov 24 17:56 EET |                     |
| service | task-message-service | minikube | oleksandr | v1.34.0 | 06 Nov 24 18:03 EET |                     |
| service | task-message         | minikube | oleksandr | v1.34.0 | 06 Nov 24 18:05 EET |                     |
| service | task-message-service | minikube | oleksandr | v1.34.0 | 06 Nov 24 18:05 EET |                     |
| service | task-message-service | minikube | oleksandr | v1.34.0 | 06 Nov 24 18:08 EET |                     |
| stop    |                      | minikube | oleksandr | v1.34.0 | 06 Nov 24 18:08 EET | 06 Nov 24 18:08 EET |
| start   |                      | minikube | oleksandr | v1.34.0 | 06 Nov 24 19:21 EET | 06 Nov 24 19:23 EET |
|---------|----------------------|----------|-----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/11/06 19:21:40
Running on machine: oleksandr-Lenovo-G50-70
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1106 19:21:40.149151    2775 out.go:345] Setting OutFile to fd 1 ...
I1106 19:21:40.149641    2775 out.go:397] isatty.IsTerminal(1) = true
I1106 19:21:40.149655    2775 out.go:358] Setting ErrFile to fd 2...
I1106 19:21:40.149710    2775 out.go:397] isatty.IsTerminal(2) = true
I1106 19:21:40.150447    2775 root.go:338] Updating PATH: /home/oleksandr/.minikube/bin
W1106 19:21:40.164589    2775 root.go:314] Error reading config file at /home/oleksandr/.minikube/config/config.json: open /home/oleksandr/.minikube/config/config.json: no such file or directory
I1106 19:21:40.177047    2775 out.go:352] Setting JSON to false
I1106 19:21:40.214706    2775 start.go:129] hostinfo: {"hostname":"oleksandr-Lenovo-G50-70","uptime":294,"bootTime":1730913407,"procs":250,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-45-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"3b6813cc-5e11-4116-b693-61c4f3910faa"}
I1106 19:21:40.214896    2775 start.go:139] virtualization: kvm host
I1106 19:21:40.461651    2775 out.go:177] üòÑ  minikube v1.34.0 on Ubuntu 22.04
I1106 19:21:40.544630    2775 notify.go:220] Checking for updates...
I1106 19:21:40.564709    2775 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1106 19:21:40.565067    2775 driver.go:394] Setting default libvirt URI to qemu:///system
I1106 19:21:42.323000    2775 docker.go:123] docker version: linux-27.3.1:Docker Engine - Community
I1106 19:21:42.323194    2775 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1106 19:21:53.350193    2775 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (11.026962767s)
I1106 19:21:53.351024    2775 info.go:266] docker info: {ID:a7f991c4-5596-4090-8321-22a8d2a5c94a Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:39 SystemTime:2024-11-06 19:21:53.332788066 +0200 EET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-45-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6099181568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:oleksandr-Lenovo-G50-70 Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7]] Warnings:<nil>}}
I1106 19:21:53.351149    2775 docker.go:318] overlay module found
I1106 19:21:53.416809    2775 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1106 19:21:53.472180    2775 start.go:297] selected driver: docker
I1106 19:21:53.472206    2775 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/oleksandr:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1106 19:21:53.472437    2775 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1106 19:21:53.472776    2775 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1106 19:21:53.657213    2775 info.go:266] docker info: {ID:a7f991c4-5596-4090-8321-22a8d2a5c94a Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:39 SystemTime:2024-11-06 19:21:53.640947776 +0200 EET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-45-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6099181568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:oleksandr-Lenovo-G50-70 Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7]] Warnings:<nil>}}
I1106 19:21:53.658169    2775 cni.go:84] Creating CNI manager for ""
I1106 19:21:53.658186    2775 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1106 19:21:53.658258    2775 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/oleksandr:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1106 19:21:53.716269    2775 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1106 19:21:53.749578    2775 cache.go:121] Beginning downloading kic base image for docker with docker
I1106 19:21:53.794194    2775 out.go:177] üöú  Pulling base image v0.0.45 ...
I1106 19:21:53.827544    2775 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local docker daemon
I1106 19:21:53.827563    2775 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1106 19:21:53.827742    2775 preload.go:146] Found local preload: /home/oleksandr/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1106 19:21:53.827768    2775 cache.go:56] Caching tarball of preloaded images
I1106 19:21:53.828441    2775 preload.go:172] Found /home/oleksandr/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1106 19:21:53.828477    2775 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1106 19:21:53.828795    2775 profile.go:143] Saving config to /home/oleksandr/.minikube/profiles/minikube/config.json ...
W1106 19:21:53.901796    2775 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45 is of wrong architecture
I1106 19:21:53.901823    2775 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45 to local cache
I1106 19:21:53.902281    2775 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory
I1106 19:21:53.915462    2775 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory, skipping pull
I1106 19:21:53.915486    2775 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45 exists in cache, skipping pull
I1106 19:21:53.915521    2775 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45 as a tarball
I1106 19:21:53.915534    2775 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45 from local cache
I1106 19:21:55.768298    2775 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45 from cached tarball
I1106 19:21:55.768334    2775 cache.go:194] Successfully downloaded all kic artifacts
I1106 19:21:55.768388    2775 start.go:360] acquireMachinesLock for minikube: {Name:mk09c48dab99164f0aa03baba35415b2784bf387 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1106 19:21:55.768568    2775 start.go:364] duration metric: took 150.947¬µs to acquireMachinesLock for "minikube"
I1106 19:21:55.768599    2775 start.go:96] Skipping create...Using existing machine configuration
I1106 19:21:55.768606    2775 fix.go:54] fixHost starting: 
I1106 19:21:55.769201    2775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1106 19:21:55.855420    2775 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1106 19:21:55.855453    2775 fix.go:138] unexpected machine state, will restart: <nil>
I1106 19:21:55.899869    2775 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1106 19:21:55.933178    2775 cli_runner.go:164] Run: docker start minikube
I1106 19:21:58.770760    2775 cli_runner.go:217] Completed: docker start minikube: (2.837513442s)
I1106 19:21:58.771026    2775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1106 19:21:58.799707    2775 kic.go:430] container "minikube" state is running.
I1106 19:21:58.800285    2775 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1106 19:21:58.827432    2775 profile.go:143] Saving config to /home/oleksandr/.minikube/profiles/minikube/config.json ...
I1106 19:21:58.827863    2775 machine.go:93] provisionDockerMachine start ...
I1106 19:21:58.828010    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:21:58.855511    2775 main.go:141] libmachine: Using SSH client type: native
I1106 19:21:58.877431    2775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1106 19:21:58.877469    2775 main.go:141] libmachine: About to run SSH command:
hostname
I1106 19:21:58.879859    2775 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59776->127.0.0.1:32768: read: connection reset by peer
I1106 19:22:01.881350    2775 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:48038->127.0.0.1:32768: read: connection reset by peer
I1106 19:22:05.862098    2775 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1106 19:22:05.862135    2775 ubuntu.go:169] provisioning hostname "minikube"
I1106 19:22:05.862339    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:05.891120    2775 main.go:141] libmachine: Using SSH client type: native
I1106 19:22:05.891583    2775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1106 19:22:05.891597    2775 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1106 19:22:07.376511    2775 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1106 19:22:07.376770    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:07.404019    2775 main.go:141] libmachine: Using SSH client type: native
I1106 19:22:07.404296    2775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1106 19:22:07.404318    2775 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1106 19:22:07.565474    2775 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1106 19:22:07.565515    2775 ubuntu.go:175] set auth options {CertDir:/home/oleksandr/.minikube CaCertPath:/home/oleksandr/.minikube/certs/ca.pem CaPrivateKeyPath:/home/oleksandr/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/oleksandr/.minikube/machines/server.pem ServerKeyPath:/home/oleksandr/.minikube/machines/server-key.pem ClientKeyPath:/home/oleksandr/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/oleksandr/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/oleksandr/.minikube}
I1106 19:22:07.565548    2775 ubuntu.go:177] setting up certificates
I1106 19:22:07.565561    2775 provision.go:84] configureAuth start
I1106 19:22:07.565653    2775 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1106 19:22:07.594506    2775 provision.go:143] copyHostCerts
I1106 19:22:07.614123    2775 exec_runner.go:144] found /home/oleksandr/.minikube/ca.pem, removing ...
I1106 19:22:07.614178    2775 exec_runner.go:203] rm: /home/oleksandr/.minikube/ca.pem
I1106 19:22:07.614315    2775 exec_runner.go:151] cp: /home/oleksandr/.minikube/certs/ca.pem --> /home/oleksandr/.minikube/ca.pem (1086 bytes)
I1106 19:22:07.767270    2775 exec_runner.go:144] found /home/oleksandr/.minikube/cert.pem, removing ...
I1106 19:22:07.767306    2775 exec_runner.go:203] rm: /home/oleksandr/.minikube/cert.pem
I1106 19:22:07.767466    2775 exec_runner.go:151] cp: /home/oleksandr/.minikube/certs/cert.pem --> /home/oleksandr/.minikube/cert.pem (1131 bytes)
I1106 19:22:07.767996    2775 exec_runner.go:144] found /home/oleksandr/.minikube/key.pem, removing ...
I1106 19:22:07.768012    2775 exec_runner.go:203] rm: /home/oleksandr/.minikube/key.pem
I1106 19:22:07.768108    2775 exec_runner.go:151] cp: /home/oleksandr/.minikube/certs/key.pem --> /home/oleksandr/.minikube/key.pem (1675 bytes)
I1106 19:22:07.900156    2775 provision.go:117] generating server cert: /home/oleksandr/.minikube/machines/server.pem ca-key=/home/oleksandr/.minikube/certs/ca.pem private-key=/home/oleksandr/.minikube/certs/ca-key.pem org=oleksandr.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1106 19:22:08.719412    2775 provision.go:177] copyRemoteCerts
I1106 19:22:08.719688    2775 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1106 19:22:08.719778    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:08.749303    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:08.993536    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I1106 19:22:09.440413    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1106 19:22:09.655149    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I1106 19:22:09.709271    2775 provision.go:87] duration metric: took 2.143692549s to configureAuth
I1106 19:22:09.709296    2775 ubuntu.go:193] setting minikube options for container-runtime
I1106 19:22:09.709613    2775 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1106 19:22:09.709717    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:09.776073    2775 main.go:141] libmachine: Using SSH client type: native
I1106 19:22:09.776387    2775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1106 19:22:09.776424    2775 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1106 19:22:09.985360    2775 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1106 19:22:09.985385    2775 ubuntu.go:71] root file system type: overlay
I1106 19:22:09.985795    2775 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1106 19:22:09.985919    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:10.015399    2775 main.go:141] libmachine: Using SSH client type: native
I1106 19:22:10.015748    2775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1106 19:22:10.015884    2775 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1106 19:22:10.212602    2775 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1106 19:22:10.212780    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:10.243021    2775 main.go:141] libmachine: Using SSH client type: native
I1106 19:22:10.243303    2775 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1106 19:22:10.243326    2775 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1106 19:22:10.799840    2775 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1106 19:22:10.799867    2775 machine.go:96] duration metric: took 11.971989305s to provisionDockerMachine
I1106 19:22:10.799883    2775 start.go:293] postStartSetup for "minikube" (driver="docker")
I1106 19:22:10.799902    2775 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1106 19:22:10.800008    2775 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1106 19:22:10.800091    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:10.828947    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:11.803382    2775 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.003321979s)
I1106 19:22:11.803540    2775 ssh_runner.go:195] Run: cat /etc/os-release
I1106 19:22:11.811370    2775 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1106 19:22:11.811458    2775 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1106 19:22:11.811510    2775 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1106 19:22:11.811524    2775 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1106 19:22:11.811551    2775 filesync.go:126] Scanning /home/oleksandr/.minikube/addons for local assets ...
I1106 19:22:11.945490    2775 filesync.go:126] Scanning /home/oleksandr/.minikube/files for local assets ...
I1106 19:22:12.144121    2775 start.go:296] duration metric: took 1.344202908s for postStartSetup
I1106 19:22:12.144488    2775 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1106 19:22:12.144590    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:12.174477    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:12.413858    2775 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1106 19:22:12.424030    2775 fix.go:56] duration metric: took 16.655416152s for fixHost
I1106 19:22:12.424053    2775 start.go:83] releasing machines lock for "minikube", held for 16.655471633s
I1106 19:22:12.424151    2775 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1106 19:22:12.450463    2775 ssh_runner.go:195] Run: cat /version.json
I1106 19:22:12.450977    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:12.451112    2775 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1106 19:22:12.451213    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:12.486824    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:12.489781    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:12.651949    2775 ssh_runner.go:195] Run: systemctl --version
I1106 19:22:16.038380    2775 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (3.587236595s)
I1106 19:22:16.038590    2775 ssh_runner.go:235] Completed: systemctl --version: (3.38661398s)
I1106 19:22:16.038736    2775 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1106 19:22:16.047632    2775 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1106 19:22:16.384192    2775 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1106 19:22:16.384305    2775 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1106 19:22:16.405185    2775 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1106 19:22:16.405218    2775 start.go:495] detecting cgroup driver to use...
I1106 19:22:16.405320    2775 detect.go:190] detected "systemd" cgroup driver on host os
I1106 19:22:16.405505    2775 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1106 19:22:16.444999    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1106 19:22:16.537193    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1106 19:22:16.556700    2775 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1106 19:22:16.556805    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1106 19:22:16.579722    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1106 19:22:16.622254    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1106 19:22:16.643919    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1106 19:22:16.680802    2775 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1106 19:22:16.700152    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1106 19:22:16.719489    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1106 19:22:16.740055    2775 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1106 19:22:16.759387    2775 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1106 19:22:16.896257    2775 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1106 19:22:16.896350    2775 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1106 19:22:17.008210    2775 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1106 19:22:17.029661    2775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1106 19:22:17.317947    2775 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1106 19:22:17.460985    2775 start.go:495] detecting cgroup driver to use...
I1106 19:22:17.461040    2775 detect.go:190] detected "systemd" cgroup driver on host os
I1106 19:22:17.461126    2775 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1106 19:22:17.485400    2775 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1106 19:22:17.485488    2775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1106 19:22:17.639620    2775 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1106 19:22:17.676157    2775 ssh_runner.go:195] Run: which cri-dockerd
I1106 19:22:17.682141    2775 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1106 19:22:17.700055    2775 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1106 19:22:17.737906    2775 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1106 19:22:17.948502    2775 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1106 19:22:18.104107    2775 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I1106 19:22:18.104420    2775 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1106 19:22:18.202918    2775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1106 19:22:18.353796    2775 ssh_runner.go:195] Run: sudo systemctl restart docker
I1106 19:22:27.990496    2775 ssh_runner.go:235] Completed: sudo systemctl restart docker: (9.636658552s)
I1106 19:22:27.990727    2775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1106 19:22:28.019942    2775 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1106 19:22:28.044945    2775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1106 19:22:28.068604    2775 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1106 19:22:28.221071    2775 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1106 19:22:28.368084    2775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1106 19:22:28.510907    2775 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1106 19:22:28.546069    2775 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1106 19:22:28.567608    2775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1106 19:22:28.715028    2775 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1106 19:22:32.211425    2775 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (3.496357543s)
I1106 19:22:32.211457    2775 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1106 19:22:32.211561    2775 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1106 19:22:32.220029    2775 start.go:563] Will wait 60s for crictl version
I1106 19:22:32.220146    2775 ssh_runner.go:195] Run: which crictl
I1106 19:22:32.227223    2775 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1106 19:22:33.575407    2775 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.348137052s)
I1106 19:22:33.575443    2775 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1106 19:22:33.575542    2775 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1106 19:22:35.165542    2775 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (1.58996827s)
I1106 19:22:35.165615    2775 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1106 19:22:35.445645    2775 out.go:235] üê≥  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1106 19:22:35.446157    2775 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1106 19:22:35.477337    2775 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1106 19:22:35.483951    2775 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1106 19:22:35.522009    2775 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/oleksandr:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1106 19:22:35.522162    2775 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1106 19:22:35.522243    2775 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1106 19:22:35.553885    2775 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1106 19:22:35.553924    2775 docker.go:615] Images already preloaded, skipping extraction
I1106 19:22:35.554009    2775 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1106 19:22:35.595923    2775 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1106 19:22:35.595943    2775 cache_images.go:84] Images are preloaded, skipping loading
I1106 19:22:35.595966    2775 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1106 19:22:35.596142    2775 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1106 19:22:35.596224    2775 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1106 19:22:38.563463    2775 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (2.967206143s)
I1106 19:22:38.563587    2775 cni.go:84] Creating CNI manager for ""
I1106 19:22:38.563604    2775 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1106 19:22:38.563627    2775 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1106 19:22:38.563658    2775 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1106 19:22:38.563868    2775 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1106 19:22:38.563950    2775 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1106 19:22:38.635293    2775 binaries.go:44] Found k8s binaries, skipping transfer
I1106 19:22:38.635420    2775 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1106 19:22:38.657432    2775 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1106 19:22:38.702272    2775 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1106 19:22:38.740541    2775 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2149 bytes)
I1106 19:22:38.793948    2775 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1106 19:22:38.802309    2775 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1106 19:22:38.827593    2775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1106 19:22:38.976332    2775 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1106 19:22:39.012116    2775 certs.go:68] Setting up /home/oleksandr/.minikube/profiles/minikube for IP: 192.168.49.2
I1106 19:22:39.012133    2775 certs.go:194] generating shared ca certs ...
I1106 19:22:39.012164    2775 certs.go:226] acquiring lock for ca certs: {Name:mk71cc58b6f91c84ce6e177478c5b6d84f776fb5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1106 19:22:39.026977    2775 certs.go:235] skipping valid "minikubeCA" ca cert: /home/oleksandr/.minikube/ca.key
I1106 19:22:39.027720    2775 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/oleksandr/.minikube/proxy-client-ca.key
I1106 19:22:39.027742    2775 certs.go:256] generating profile certs ...
I1106 19:22:39.060659    2775 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/oleksandr/.minikube/profiles/minikube/client.key
I1106 19:22:39.061278    2775 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/oleksandr/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1106 19:22:39.094296    2775 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/oleksandr/.minikube/profiles/minikube/proxy-client.key
I1106 19:22:39.094985    2775 certs.go:484] found cert: /home/oleksandr/.minikube/certs/ca-key.pem (1675 bytes)
I1106 19:22:39.095097    2775 certs.go:484] found cert: /home/oleksandr/.minikube/certs/ca.pem (1086 bytes)
I1106 19:22:39.095180    2775 certs.go:484] found cert: /home/oleksandr/.minikube/certs/cert.pem (1131 bytes)
I1106 19:22:39.095256    2775 certs.go:484] found cert: /home/oleksandr/.minikube/certs/key.pem (1675 bytes)
I1106 19:22:39.120799    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1106 19:22:39.192412    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1106 19:22:39.276122    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1106 19:22:39.332412    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1106 19:22:39.383546    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1106 19:22:39.497047    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1106 19:22:39.562617    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1106 19:22:39.665909    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1106 19:22:39.735074    2775 ssh_runner.go:362] scp /home/oleksandr/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1106 19:22:39.793288    2775 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1106 19:22:39.866901    2775 ssh_runner.go:195] Run: openssl version
I1106 19:22:40.071360    2775 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1106 19:22:40.134267    2775 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1106 19:22:40.142592    2775 certs.go:528] hashing: -rw-r--r--. 1 root root 1111 Nov  6 15:39 /usr/share/ca-certificates/minikubeCA.pem
I1106 19:22:40.142738    2775 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1106 19:22:40.158821    2775 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1106 19:22:40.179763    2775 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1106 19:22:40.187519    2775 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1106 19:22:40.248148    2775 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1106 19:22:40.312961    2775 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1106 19:22:40.369128    2775 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1106 19:22:40.402079    2775 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1106 19:22:40.418431    2775 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1106 19:22:40.433020    2775 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/oleksandr:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1106 19:22:40.433210    2775 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1106 19:22:40.464585    2775 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1106 19:22:40.498107    2775 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1106 19:22:40.498122    2775 kubeadm.go:593] restartPrimaryControlPlane start ...
I1106 19:22:40.498212    2775 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1106 19:22:40.575105    2775 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1106 19:22:40.629993    2775 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/oleksandr/.kube/config
I1106 19:22:40.630445    2775 kubeconfig.go:62] /home/oleksandr/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I1106 19:22:40.631186    2775 lock.go:35] WriteFile acquiring /home/oleksandr/.kube/config: {Name:mk12568286b261514faceac9349e37ea7a0e71ea Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1106 19:22:40.959757    2775 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1106 19:22:41.004588    2775 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I1106 19:22:41.004629    2775 kubeadm.go:597] duration metric: took 506.496228ms to restartPrimaryControlPlane
I1106 19:22:41.004649    2775 kubeadm.go:394] duration metric: took 571.637373ms to StartCluster
I1106 19:22:41.004699    2775 settings.go:142] acquiring lock: {Name:mk86f4387acadca2585c2c2c1e46cdf9a6115fb1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1106 19:22:41.004896    2775 settings.go:150] Updating kubeconfig:  /home/oleksandr/.kube/config
I1106 19:22:41.006015    2775 lock.go:35] WriteFile acquiring /home/oleksandr/.kube/config: {Name:mk12568286b261514faceac9349e37ea7a0e71ea Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1106 19:22:41.006405    2775 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1106 19:22:41.006571    2775 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1106 19:22:41.006764    2775 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1106 19:22:41.006819    2775 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1106 19:22:41.006841    2775 addons.go:243] addon storage-provisioner should already be in state true
I1106 19:22:41.006881    2775 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1106 19:22:41.006902    2775 host.go:66] Checking if "minikube" exists ...
I1106 19:22:41.006929    2775 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1106 19:22:41.006981    2775 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1106 19:22:41.007440    2775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1106 19:22:41.008216    2775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1106 19:22:41.169496    2775 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1106 19:22:41.169523    2775 addons.go:243] addon default-storageclass should already be in state true
I1106 19:22:41.169583    2775 host.go:66] Checking if "minikube" exists ...
I1106 19:22:41.170723    2775 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1106 19:22:41.197829    2775 out.go:177] üîé  Verifying Kubernetes components...
I1106 19:22:41.200713    2775 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1106 19:22:41.200729    2775 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1106 19:22:41.200804    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:41.228873    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:41.265574    2775 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1106 19:22:41.343260    2775 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1106 19:22:41.421117    2775 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1106 19:22:41.421136    2775 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1106 19:22:41.421241    2775 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1106 19:22:41.442467    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:22:41.456646    2775 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/oleksandr/.minikube/machines/minikube/id_rsa Username:docker}
I1106 19:22:41.515118    2775 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1106 19:22:41.626025    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1106 19:22:48.298123    2775 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.855607088s)
W1106 19:22:48.298173    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.298205    2775 retry.go:31] will retry after 241.389299ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.298303    2775 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (6.783150312s)
I1106 19:22:48.298362    2775 api_server.go:52] waiting for apiserver process to appear ...
I1106 19:22:48.298486    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:48.299053    2775 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.672987908s)
W1106 19:22:48.299083    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.299098    2775 retry.go:31] will retry after 308.108388ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.539872    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:22:48.607730    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:22:48.677509    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.677540    2775 retry.go:31] will retry after 486.712101ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1106 19:22:48.739042    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.739063    2775 retry.go:31] will retry after 224.523859ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:48.799383    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:48.964789    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:22:49.095476    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.095501    2775 retry.go:31] will retry after 330.325192ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.164914    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:22:49.298951    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1106 19:22:49.410934    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.410968    2775 retry.go:31] will retry after 349.283219ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.426885    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:22:49.548747    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.548772    2775 retry.go:31] will retry after 655.274501ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.761068    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:22:49.798903    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1106 19:22:49.878554    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:49.878582    2775 retry.go:31] will retry after 1.192637792s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:50.205270    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1106 19:22:50.299018    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1106 19:22:50.310293    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:50.310319    2775 retry.go:31] will retry after 1.53493203s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:50.799221    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:51.071650    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1106 19:22:51.178162    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:51.178194    2775 retry.go:31] will retry after 1.709985101s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:51.299551    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:51.798852    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:51.846356    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:22:51.952343    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:51.952368    2775 retry.go:31] will retry after 2.587573246s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:52.299640    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:52.799044    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:52.888729    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1106 19:22:53.076141    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:53.076175    2775 retry.go:31] will retry after 1.630890431s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:53.298602    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:53.799494    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:54.298764    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:54.540320    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:22:54.681024    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:54.681048    2775 retry.go:31] will retry after 3.974591442s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:54.708254    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:22:54.798780    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1106 19:22:54.814623    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:54.814652    2775 retry.go:31] will retry after 3.286019951s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:55.298939    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:55.798555    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:56.298641    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:56.799636    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:57.298951    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:57.798911    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:58.101870    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1106 19:22:58.258522    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:58.258553    2775 retry.go:31] will retry after 2.453819847s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:58.298906    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:58.656353    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1106 19:22:58.798933    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1106 19:22:58.876268    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:58.876296    2775 retry.go:31] will retry after 4.138160077s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:22:59.299171    2775 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1106 19:22:59.331124    2775 api_server.go:72] duration metric: took 18.324670259s to wait for apiserver process to appear ...
I1106 19:22:59.331144    2775 api_server.go:88] waiting for apiserver healthz status ...
I1106 19:22:59.331169    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:22:59.331726    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:22:59.831597    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:22:59.832500    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:00.331316    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:00.332155    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:00.713020    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:23:00.831823    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:00.832318    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W1106 19:23:00.842534    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:00.842560    2775 retry.go:31] will retry after 8.34181929s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:01.332259    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:01.333220    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:01.832138    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:01.833295    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:02.331995    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:02.332882    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:02.831411    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:02.832499    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:03.014795    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:23:03.125492    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:03.125522    2775 retry.go:31] will retry after 9.213285626s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:03.331912    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:03.332963    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:03.831410    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:03.832406    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:04.331893    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:04.332821    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:04.831346    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:04.832395    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:05.332126    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:05.333232    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:05.831828    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:05.832653    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:06.332314    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:06.333282    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:06.831900    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:06.832798    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:07.331879    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:07.332698    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:07.831999    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:07.832868    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:08.331401    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:08.332846    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:08.832012    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:08.832886    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:09.184753    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1106 19:23:09.294640    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:09.294690    2775 retry.go:31] will retry after 5.881210502s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:09.331925    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:09.333013    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:09.831520    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:09.832361    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:10.331955    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:10.332808    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:10.831574    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:10.832521    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:11.332161    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:11.333114    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:11.831574    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:11.832136    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:12.331288    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:12.331880    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:12.338980    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1106 19:23:12.474237    2775 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:12.474264    2775 retry.go:31] will retry after 9.904223784s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1106 19:23:12.831299    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:12.831924    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:13.331540    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:13.332523    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:13.832191    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:13.833311    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:14.331759    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:14.332603    2775 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1106 19:23:14.831821    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:15.177103    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1106 19:23:19.171049    2775 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1106 19:23:19.171094    2775 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1106 19:23:19.171115    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:19.265564    2775 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1106 19:23:19.265592    2775 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1106 19:23:19.331970    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:19.345844    2775 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1106 19:23:19.345876    2775 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1106 19:23:19.831549    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:19.839528    2775 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (4.662378851s)
I1106 19:23:19.842604    2775 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1106 19:23:19.842637    2775 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1106 19:23:20.331968    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:20.339503    2775 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1106 19:23:20.339531    2775 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1106 19:23:20.831483    2775 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1106 19:23:20.846577    2775 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1106 19:23:20.849007    2775 api_server.go:141] control plane version: v1.31.0
I1106 19:23:20.849039    2775 api_server.go:131] duration metric: took 21.517883016s to wait for apiserver health ...
I1106 19:23:20.849071    2775 system_pods.go:43] waiting for kube-system pods to appear ...
I1106 19:23:20.887285    2775 system_pods.go:59] 7 kube-system pods found
I1106 19:23:20.887342    2775 system_pods.go:61] "coredns-6f6b679f8f-k6wgh" [06685d03-4af8-4b40-b5ba-684b51949919] Running
I1106 19:23:20.887367    2775 system_pods.go:61] "etcd-minikube" [0f272d58-6b10-40d3-b711-e333417919b9] Running
I1106 19:23:20.887385    2775 system_pods.go:61] "kube-apiserver-minikube" [5d7c05be-0cf1-45fa-8229-adc41a2bdbba] Running
I1106 19:23:20.887401    2775 system_pods.go:61] "kube-controller-manager-minikube" [d611bd89-1d2b-4549-a6a7-2a855c0106d3] Running
I1106 19:23:20.887415    2775 system_pods.go:61] "kube-proxy-jrz7p" [0be96ac5-8886-421f-86b2-5c2314ac1ab4] Running
I1106 19:23:20.887429    2775 system_pods.go:61] "kube-scheduler-minikube" [1519e7aa-8884-4713-942e-481dbe41facd] Running
I1106 19:23:20.887442    2775 system_pods.go:61] "storage-provisioner" [aff9f9af-b0a7-4d02-993d-749513a63c7e] Running
I1106 19:23:20.887462    2775 system_pods.go:74] duration metric: took 38.373281ms to wait for pod list to return data ...
I1106 19:23:20.887497    2775 kubeadm.go:582] duration metric: took 39.881034275s to wait for: map[apiserver:true system_pods:true]
I1106 19:23:20.887535    2775 node_conditions.go:102] verifying NodePressure condition ...
I1106 19:23:20.904844    2775 node_conditions.go:122] node storage ephemeral capacity is 95011676Ki
I1106 19:23:20.904890    2775 node_conditions.go:123] node cpu capacity is 4
I1106 19:23:20.904940    2775 node_conditions.go:105] duration metric: took 17.393913ms to run NodePressure ...
I1106 19:23:20.904971    2775 start.go:241] waiting for startup goroutines ...
I1106 19:23:22.379063    2775 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1106 19:23:24.900777    2775 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.5216752s)
I1106 19:23:25.165848    2775 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I1106 19:23:25.309754    2775 addons.go:510] duration metric: took 44.303088434s for enable addons: enabled=[default-storageclass storage-provisioner]
I1106 19:23:25.309847    2775 start.go:246] waiting for cluster config update ...
I1106 19:23:25.309896    2775 start.go:255] writing updated cluster config ...
I1106 19:23:25.310824    2775 ssh_runner.go:195] Run: rm -f paused
I1106 19:23:46.325166    2775 start.go:600] kubectl: 1.31.1, cluster: 1.31.0 (minor skew: 0)
I1106 19:23:46.486600    2775 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 06 17:22:16 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 06 17:22:18 minikube systemd[1]: docker.service: Deactivated successfully.
Nov 06 17:22:18 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 06 17:22:18 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 06 17:22:23 minikube dockerd[534]: time="2024-11-06T17:22:23.195308320Z" level=info msg="Starting up"
Nov 06 17:22:24 minikube dockerd[534]: time="2024-11-06T17:22:24.198969816Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 06 17:22:25 minikube dockerd[534]: time="2024-11-06T17:22:25.406983663Z" level=info msg="Loading containers: start."
Nov 06 17:22:26 minikube dockerd[534]: time="2024-11-06T17:22:26.741516288Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Nov 06 17:22:26 minikube dockerd[534]: time="2024-11-06T17:22:26.989314449Z" level=info msg="Loading containers: done."
Nov 06 17:22:27 minikube dockerd[534]: time="2024-11-06T17:22:27.585889244Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Nov 06 17:22:27 minikube dockerd[534]: time="2024-11-06T17:22:27.594316619Z" level=info msg="Daemon has completed initialization"
Nov 06 17:22:27 minikube dockerd[534]: time="2024-11-06T17:22:27.984323506Z" level=info msg="API listen on /var/run/docker.sock"
Nov 06 17:22:27 minikube systemd[1]: Started Docker Application Container Engine.
Nov 06 17:22:27 minikube dockerd[534]: time="2024-11-06T17:22:27.984432804Z" level=info msg="API listen on [::]:2376"
Nov 06 17:22:28 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 06 17:22:31 minikube cri-dockerd[819]: time="2024-11-06T17:22:31Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 06 17:22:31 minikube cri-dockerd[819]: time="2024-11-06T17:22:31Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 06 17:22:31 minikube cri-dockerd[819]: time="2024-11-06T17:22:31Z" level=info msg="Start docker client with request timeout 0s"
Nov 06 17:22:31 minikube cri-dockerd[819]: time="2024-11-06T17:22:31Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 06 17:22:32 minikube cri-dockerd[819]: time="2024-11-06T17:22:32Z" level=info msg="Loaded network plugin cni"
Nov 06 17:22:32 minikube cri-dockerd[819]: time="2024-11-06T17:22:32Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 06 17:22:32 minikube cri-dockerd[819]: time="2024-11-06T17:22:32Z" level=info msg="Setting cgroupDriver systemd"
Nov 06 17:22:32 minikube cri-dockerd[819]: time="2024-11-06T17:22:32Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 06 17:22:32 minikube cri-dockerd[819]: time="2024-11-06T17:22:32Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 06 17:22:32 minikube cri-dockerd[819]: time="2024-11-06T17:22:32Z" level=info msg="Start cri-dockerd grpc backend"
Nov 06 17:22:32 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 06 17:22:49 minikube cri-dockerd[819]: time="2024-11-06T17:22:49Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"task-message-8576c8bf94-nwdj8_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"db1a0ce4ba0681c95e6525ef98d90bf36c6842d56581f77b4602a997532f99ac\""
Nov 06 17:22:49 minikube cri-dockerd[819]: time="2024-11-06T17:22:49Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-k6wgh_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c2fc2afd8b45965c4a7b4734305267b80555219029488a4570516793b930d68d\""
Nov 06 17:22:55 minikube cri-dockerd[819]: time="2024-11-06T17:22:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/261d263df6fa3d16a8467c4c8cef8c4c6cd64151b9f47359ffc823fadf864a18/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 06 17:22:56 minikube cri-dockerd[819]: time="2024-11-06T17:22:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b502d5c08fdcaa7553d3cb57ce832978587674e859c40ca924ecb11eb303cb1f/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 06 17:22:56 minikube cri-dockerd[819]: time="2024-11-06T17:22:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9404d66f2df977895fa3479f7ce62831a2c1f82ebc4fc34efe301141c3c54d0e/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Nov 06 17:22:56 minikube cri-dockerd[819]: time="2024-11-06T17:22:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf51a149956401f6301852f2503b7c35ea8821ecea1c3f41f9ad35d44291f78e/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Nov 06 17:23:19 minikube cri-dockerd[819]: time="2024-11-06T17:23:19Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 06 17:23:29 minikube cri-dockerd[819]: time="2024-11-06T17:23:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/05f314e9f0ea1abe90dffad8b4e20357a208e2449bf577062136645698b1271d/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 06 17:23:29 minikube cri-dockerd[819]: time="2024-11-06T17:23:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1e9c75f26c5e1aa910d3a3a4278bfcb4a57494dbcbfb39bf7366fed26873e5ab/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Nov 06 17:23:32 minikube cri-dockerd[819]: time="2024-11-06T17:23:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a9319aa738e22e6314db50cb171b5fe339ecbf92b4912fc7b0c6ec8b03484dda/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 06 17:23:32 minikube cri-dockerd[819]: time="2024-11-06T17:23:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/06d4a50bb48a6dee1ff28f85966af3be25d5e66df7dea2ea4a99c29e9906f3bf/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Nov 06 17:23:45 minikube dockerd[534]: time="2024-11-06T17:23:45.826833891Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 06 17:23:45 minikube dockerd[534]: time="2024-11-06T17:23:45.826934208Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 06 17:24:01 minikube dockerd[534]: time="2024-11-06T17:24:01.907245511Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 06 17:24:01 minikube dockerd[534]: time="2024-11-06T17:24:01.907920508Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 06 17:24:14 minikube dockerd[534]: time="2024-11-06T17:24:14.622287566Z" level=info msg="ignoring event" container=4034c95d57681f98724b47bd8355a12f0dbafa74c2e788ffb527fcb975e3d8dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 06 17:24:29 minikube dockerd[534]: time="2024-11-06T17:24:29.931520262Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 06 17:24:29 minikube dockerd[534]: time="2024-11-06T17:24:29.931726125Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 06 17:25:12 minikube dockerd[534]: time="2024-11-06T17:25:12.839349610Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 06 17:25:12 minikube dockerd[534]: time="2024-11-06T17:25:12.839522261Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
32155398fa82b       6e38f40d628db       46 seconds ago       Running             storage-provisioner       2                   05f314e9f0ea1       storage-provisioner
b9116361b0604       cbb01a7bd410d       About a minute ago   Running             coredns                   1                   06d4a50bb48a6       coredns-6f6b679f8f-k6wgh
c7f546976bf76       ad83b2ca7b09e       About a minute ago   Running             kube-proxy                1                   1e9c75f26c5e1       kube-proxy-jrz7p
4034c95d57681       6e38f40d628db       About a minute ago   Exited              storage-provisioner       1                   05f314e9f0ea1       storage-provisioner
221506fed780a       604f5db92eaa8       2 minutes ago        Running             kube-apiserver            1                   bf51a14995640       kube-apiserver-minikube
cacd0f5b2b29a       045733566833c       2 minutes ago        Running             kube-controller-manager   1                   b502d5c08fdca       kube-controller-manager-minikube
6d6f3018d6c78       2e96e5913fc06       2 minutes ago        Running             etcd                      1                   9404d66f2df97       etcd-minikube
77eef5262a3d7       1766f54c897f0       2 minutes ago        Running             kube-scheduler            1                   261d263df6fa3       kube-scheduler-minikube
9a429340af6be       cbb01a7bd410d       2 hours ago          Exited              coredns                   0                   c2fc2afd8b459       coredns-6f6b679f8f-k6wgh
0f4c8dc4c091d       ad83b2ca7b09e       2 hours ago          Exited              kube-proxy                0                   edd0dbfd3d253       kube-proxy-jrz7p
ed41c10b6569f       604f5db92eaa8       2 hours ago          Exited              kube-apiserver            0                   aa9fc2e4797f3       kube-apiserver-minikube
cdc5852872fba       1766f54c897f0       2 hours ago          Exited              kube-scheduler            0                   d7dbb6864573f       kube-scheduler-minikube
9d1a229c7fb2a       2e96e5913fc06       2 hours ago          Exited              etcd                      0                   515f5b7b2c441       etcd-minikube
1c59ab2508c11       045733566833c       2 hours ago          Exited              kube-controller-manager   0                   c7b93f127c5b8       kube-controller-manager-minikube


==> coredns [9a429340af6b] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:46795 - 15343 "HINFO IN 8747062846031764771.7290154139836524371. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.046496083s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1789823358]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (06-Nov-2024 15:40:39.620) (total time: 30001ms):
Trace[1789823358]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (15:41:09.621)
Trace[1789823358]: [30.001492048s] [30.001492048s] END
[INFO] plugin/kubernetes: Trace[1210420453]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (06-Nov-2024 15:40:39.620) (total time: 30001ms):
Trace[1210420453]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (15:41:09.621)
Trace[1210420453]: [30.001526141s] [30.001526141s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[441826360]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (06-Nov-2024 15:40:39.620) (total time: 30001ms):
Trace[441826360]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (15:41:09.621)
Trace[441826360]: [30.001824493s] [30.001824493s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [b9116361b060] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:34979 - 19597 "HINFO IN 7972788965133809394.1651786045063360834. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.124368074s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[347929964]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (06-Nov-2024 17:23:48.335) (total time: 30069ms):
Trace[347929964]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30051ms (17:24:18.386)
Trace[347929964]: [30.069535908s] [30.069535908s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1547699432]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (06-Nov-2024 17:23:48.335) (total time: 30069ms):
Trace[1547699432]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30053ms (17:24:18.388)
Trace[1547699432]: [30.069704225s] [30.069704225s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[485650010]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (06-Nov-2024 17:23:48.335) (total time: 30069ms):
Trace[485650010]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30053ms (17:24:18.388)
Trace[485650010]: [30.069982454s] [30.069982454s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_06T17_40_13_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 06 Nov 2024 15:40:01 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 06 Nov 2024 17:25:22 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 06 Nov 2024 17:23:19 +0000   Wed, 06 Nov 2024 15:40:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 06 Nov 2024 17:23:19 +0000   Wed, 06 Nov 2024 15:40:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 06 Nov 2024 17:23:19 +0000   Wed, 06 Nov 2024 15:40:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 06 Nov 2024 17:23:19 +0000   Wed, 06 Nov 2024 15:40:01 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  95011676Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5956232Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  95011676Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5956232Ki
  pods:               110
System Info:
  Machine ID:                 8ec43e6a39c74630ad03ab8fc63bdb29
  System UUID:                77455fcd-0820-460a-b8d5-549df8b57bcb
  Boot ID:                    51adc915-eb47-4670-ba84-31f260fac2fb
  Kernel Version:             6.8.0-45-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     task-message-8576c8bf94-nwdj8       0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m
  kube-system                 coredns-6f6b679f8f-k6wgh            100m (2%)     0 (0%)      70Mi (1%)        170Mi (2%)     105m
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         105m
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         105m
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         105m
  kube-system                 kube-proxy-jrz7p                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         105m
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         105m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         104m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 104m                   kube-proxy       
  Normal  Starting                 99s                    kube-proxy       
  Normal  NodeHasNoDiskPressure    105m (x8 over 105m)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  105m (x8 over 105m)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  Starting                 105m                   kubelet          Starting kubelet.
  Normal  NodeHasSufficientPID     105m (x7 over 105m)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  105m                   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     105m                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  105m                   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  105m                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    105m                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 105m                   kubelet          Starting kubelet.
  Normal  RegisteredNode           105m                   node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 2m39s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  2m39s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  2m38s (x8 over 2m39s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m38s (x8 over 2m39s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m38s (x7 over 2m39s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           2m6s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000175] audit: error in audit_log_object_context
[  +0.000023] audit: error in audit_log_object_context
[  +0.636495] audit: error in audit_log_object_context
[  +0.000543] audit: error in audit_log_object_context
[  +0.000442] audit: error in audit_log_object_context
[  +2.660028] SELinux: security_context_str_to_sid (system_u:object_r:snappy_snap_t:s0) failed with errno=-22
[  +2.390673] audit_panic: 4 callbacks suppressed
[  +0.000007] audit: error in audit_log_object_context
[  +0.198844] audit: error in audit_log_object_context
[  +0.449599] audit: error in audit_log_object_context
[  +0.000024] audit: error in audit_log_object_context
[  +0.000010] audit: error in audit_log_object_context
[  +0.000157] audit: error in audit_log_object_context
[  +0.000023] audit: error in audit_log_object_context
[  +0.628742] audit: error in audit_log_object_context
[  +0.000616] audit: error in audit_log_object_context
[  +0.078324] audit: error in audit_log_object_context
[  +6.590252] audit_panic: 1 callbacks suppressed
[  +0.000006] audit: error in audit_log_object_context
[  +2.693531] audit: error in audit_log_object_context
[  +1.220688] workqueue: pm_runtime_work hogged CPU for >13333us 4 times, consider switching to WQ_UNBOUND
[Nov 6 17:18] audit: error in audit_log_object_context
[  +1.130754] audit: error in audit_log_object_context
[  +0.001692] audit: error in audit_log_object_context
[  +0.000021] audit: error in audit_log_object_context
[  +0.000011] audit: error in audit_log_object_context
[  +0.000112] audit: error in audit_log_object_context
[  +0.000016] audit: error in audit_log_object_context
[  +0.052104] audit: error in audit_log_object_context
[  +0.000021] audit: error in audit_log_object_context
[ +14.176413] audit: error in audit_log_object_context
[  +4.161613] audit: error in audit_log_object_context
[  +0.000556] audit: error in audit_log_object_context
[  +0.372819] audit: error in audit_log_object_context
[  +2.265541] audit: error in audit_log_object_context
[ +11.747688] audit: error in audit_log_object_context
[  +0.861832] audit: error in audit_log_object_context
[Nov 6 17:19] audit: error in audit_log_object_context
[  +0.209477] audit: error in audit_log_object_context
[  +0.350589] audit: error in audit_log_object_context
[  +1.805343] audit: error in audit_log_object_context
[  +0.000026] audit: error in audit_log_object_context
[  +0.000012] audit: error in audit_log_object_context
[  +0.000290] audit: error in audit_log_object_context
[  +0.000014] audit: error in audit_log_object_context
[ +47.399183] audit: error in audit_log_object_context
[Nov 6 17:20] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[ +15.276449] workqueue: pm_runtime_work hogged CPU for >13333us 8 times, consider switching to WQ_UNBOUND
[Nov 6 17:21] audit: error in audit_log_object_context
[  +2.522911] audit: error in audit_log_object_context
[  +0.000514] audit: error in audit_log_object_context
[  +0.000018] audit: error in audit_log_object_context
[  +0.000010] audit: error in audit_log_object_context
[  +0.000154] audit: error in audit_log_object_context
[  +0.000012] audit: error in audit_log_object_context
[Nov 6 17:22] audit: error in audit_log_object_context
[ +12.389225] audit: error in audit_log_object_context
[  +0.000704] audit: error in audit_log_object_context
[  +0.018980] SELinux: security_context_str_to_sid (system_u:object_r:snappy_snap_t:s0) failed with errno=-22
[  +9.286465] audit: error in audit_log_object_context


==> etcd [6d6f3018d6c7] <==
{"level":"warn","ts":"2024-11-06T17:23:37.283428Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.652051ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:37.283485Z","caller":"traceutil/trace.go:171","msg":"trace[383016255] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1911; }","duration":"140.716297ms","start":"2024-11-06T17:23:37.142751Z","end":"2024-11-06T17:23:37.283467Z","steps":["trace[383016255] 'agreement among raft nodes before linearized reading'  (duration: 140.515559ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:37.608578Z","caller":"traceutil/trace.go:171","msg":"trace[1281653318] transaction","detail":"{read_only:false; response_revision:1913; number_of_response:1; }","duration":"226.570999ms","start":"2024-11-06T17:23:37.381951Z","end":"2024-11-06T17:23:37.608522Z","steps":["trace[1281653318] 'process raft request'  (duration: 169.068784ms)","trace[1281653318] 'compare'  (duration: 57.215122ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T17:23:37.829727Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.735841ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033066449782780 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/storage-provisioner.180570f05032f984\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/storage-provisioner.180570f05032f984\" value_size:697 lease:8128033066449782622 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-11-06T17:23:37.829932Z","caller":"traceutil/trace.go:171","msg":"trace[513969100] linearizableReadLoop","detail":"{readStateIndex:2291; appliedIndex:2290; }","duration":"200.954474ms","start":"2024-11-06T17:23:37.628942Z","end":"2024-11-06T17:23:37.829896Z","steps":["trace[513969100] 'read index received'  (duration: 88.844123ms)","trace[513969100] 'applied index is now lower than readState.Index'  (duration: 112.106284ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:37.830199Z","caller":"traceutil/trace.go:171","msg":"trace[1893464116] transaction","detail":"{read_only:false; response_revision:1914; number_of_response:1; }","duration":"214.579181ms","start":"2024-11-06T17:23:37.615581Z","end":"2024-11-06T17:23:37.830160Z","steps":["trace[1893464116] 'process raft request'  (duration: 102.26974ms)","trace[1893464116] 'compare'  (duration: 111.549964ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T17:23:37.830333Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.37537ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:37.830448Z","caller":"traceutil/trace.go:171","msg":"trace[222494677] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1914; }","duration":"201.497446ms","start":"2024-11-06T17:23:37.628921Z","end":"2024-11-06T17:23:37.830419Z","steps":["trace[222494677] 'agreement among raft nodes before linearized reading'  (duration: 201.277219ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:38.102219Z","caller":"traceutil/trace.go:171","msg":"trace[1629018718] linearizableReadLoop","detail":"{readStateIndex:2293; appliedIndex:2292; }","duration":"170.572392ms","start":"2024-11-06T17:23:37.931598Z","end":"2024-11-06T17:23:38.102171Z","steps":["trace[1629018718] 'read index received'  (duration: 121.153356ms)","trace[1629018718] 'applied index is now lower than readState.Index'  (duration: 49.416457ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T17:23:38.102545Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.914528ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/storage-provisioner.180570f05032f984\" ","response":"range_response_count:1 size:795"}
{"level":"info","ts":"2024-11-06T17:23:38.102659Z","caller":"traceutil/trace.go:171","msg":"trace[404234465] range","detail":"{range_begin:/registry/events/kube-system/storage-provisioner.180570f05032f984; range_end:; response_count:1; response_revision:1916; }","duration":"171.044473ms","start":"2024-11-06T17:23:37.931579Z","end":"2024-11-06T17:23:38.102623Z","steps":["trace[404234465] 'agreement among raft nodes before linearized reading'  (duration: 170.773807ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:38.103198Z","caller":"traceutil/trace.go:171","msg":"trace[1025759851] transaction","detail":"{read_only:false; response_revision:1916; number_of_response:1; }","duration":"255.320909ms","start":"2024-11-06T17:23:37.847824Z","end":"2024-11-06T17:23:38.103145Z","steps":["trace[1025759851] 'process raft request'  (duration: 205.052885ms)","trace[1025759851] 'compare'  (duration: 48.494393ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:38.305062Z","caller":"traceutil/trace.go:171","msg":"trace[1040788695] transaction","detail":"{read_only:false; response_revision:1917; number_of_response:1; }","duration":"194.981623ms","start":"2024-11-06T17:23:38.110051Z","end":"2024-11-06T17:23:38.305032Z","steps":["trace[1040788695] 'process raft request'  (duration: 104.352916ms)","trace[1040788695] 'compare'  (duration: 90.510669ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:38.526469Z","caller":"traceutil/trace.go:171","msg":"trace[1788891951] transaction","detail":"{read_only:false; response_revision:1920; number_of_response:1; }","duration":"119.631527ms","start":"2024-11-06T17:23:38.406810Z","end":"2024-11-06T17:23:38.526442Z","steps":["trace[1788891951] 'process raft request'  (duration: 119.342374ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:38.806978Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.972045ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:38.807128Z","caller":"traceutil/trace.go:171","msg":"trace[1720944481] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1920; }","duration":"178.135348ms","start":"2024-11-06T17:23:38.628957Z","end":"2024-11-06T17:23:38.807093Z","steps":["trace[1720944481] 'range keys from in-memory index tree'  (duration: 177.945167ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:38.807209Z","caller":"traceutil/trace.go:171","msg":"trace[599791069] transaction","detail":"{read_only:false; response_revision:1921; number_of_response:1; }","duration":"277.302506ms","start":"2024-11-06T17:23:38.529866Z","end":"2024-11-06T17:23:38.807169Z","steps":["trace[599791069] 'process raft request'  (duration: 276.983501ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:39.002410Z","caller":"traceutil/trace.go:171","msg":"trace[1092768499] transaction","detail":"{read_only:false; response_revision:1922; number_of_response:1; }","duration":"188.327718ms","start":"2024-11-06T17:23:38.813997Z","end":"2024-11-06T17:23:39.002324Z","steps":["trace[1092768499] 'process raft request'  (duration: 97.536824ms)","trace[1092768499] 'compare'  (duration: 90.504824ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:39.246050Z","caller":"traceutil/trace.go:171","msg":"trace[1538242050] transaction","detail":"{read_only:false; response_revision:1924; number_of_response:1; }","duration":"226.608391ms","start":"2024-11-06T17:23:39.019386Z","end":"2024-11-06T17:23:39.245994Z","steps":["trace[1538242050] 'process raft request'  (duration: 154.265148ms)","trace[1538242050] 'compare'  (duration: 72.151319ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T17:23:39.477980Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.221745ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033066449782796 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/coredns-6f6b679f8f-k6wgh.180570f246ec01ee\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/coredns-6f6b679f8f-k6wgh.180570f246ec01ee\" value_size:634 lease:8128033066449782622 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-11-06T17:23:39.478419Z","caller":"traceutil/trace.go:171","msg":"trace[1599512240] transaction","detail":"{read_only:false; response_revision:1926; number_of_response:1; }","duration":"218.064491ms","start":"2024-11-06T17:23:39.260293Z","end":"2024-11-06T17:23:39.478357Z","steps":["trace[1599512240] 'process raft request'  (duration: 217.894087ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:39.478596Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"336.460499ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:39.478726Z","caller":"traceutil/trace.go:171","msg":"trace[40845461] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1926; }","duration":"336.59274ms","start":"2024-11-06T17:23:39.142107Z","end":"2024-11-06T17:23:39.478699Z","steps":["trace[40845461] 'agreement among raft nodes before linearized reading'  (duration: 336.371133ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:39.478807Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-11-06T17:23:39.142024Z","time spent":"336.760482ms","remote":"127.0.0.1:43594","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-11-06T17:23:39.478847Z","caller":"traceutil/trace.go:171","msg":"trace[833901765] transaction","detail":"{read_only:false; response_revision:1925; number_of_response:1; }","duration":"415.480262ms","start":"2024-11-06T17:23:39.063324Z","end":"2024-11-06T17:23:39.478804Z","steps":["trace[833901765] 'process raft request'  (duration: 287.335232ms)","trace[833901765] 'compare'  (duration: 127.057964ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:39.478422Z","caller":"traceutil/trace.go:171","msg":"trace[1083464758] linearizableReadLoop","detail":"{readStateIndex:2302; appliedIndex:2300; }","duration":"336.251513ms","start":"2024-11-06T17:23:39.142143Z","end":"2024-11-06T17:23:39.478394Z","steps":["trace[1083464758] 'read index received'  (duration: 31.56591ms)","trace[1083464758] 'applied index is now lower than readState.Index'  (duration: 304.682017ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T17:23:39.479010Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-11-06T17:23:39.063289Z","time spent":"415.637621ms","remote":"127.0.0.1:43668","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":722,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/coredns-6f6b679f8f-k6wgh.180570f246ec01ee\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/coredns-6f6b679f8f-k6wgh.180570f246ec01ee\" value_size:634 lease:8128033066449782622 >> failure:<>"}
{"level":"warn","ts":"2024-11-06T17:23:39.732261Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.393186ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:39.732461Z","caller":"traceutil/trace.go:171","msg":"trace[1535287875] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1926; }","duration":"106.563784ms","start":"2024-11-06T17:23:39.625816Z","end":"2024-11-06T17:23:39.732380Z","steps":["trace[1535287875] 'range keys from in-memory index tree'  (duration: 106.360679ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:39.732529Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.763505ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:39.732644Z","caller":"traceutil/trace.go:171","msg":"trace[1036788910] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1926; }","duration":"103.885913ms","start":"2024-11-06T17:23:39.628724Z","end":"2024-11-06T17:23:39.732610Z","steps":["trace[1036788910] 'range keys from in-memory index tree'  (duration: 103.734523ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:39.733016Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.564477ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033066449782800 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-proxy-jrz7p.180570f24e27c3d2\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/kube-proxy-jrz7p.180570f24e27c3d2\" value_size:624 lease:8128033066449782622 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-11-06T17:23:39.733248Z","caller":"traceutil/trace.go:171","msg":"trace[1087901862] transaction","detail":"{read_only:false; response_revision:1927; number_of_response:1; }","duration":"247.947007ms","start":"2024-11-06T17:23:39.485259Z","end":"2024-11-06T17:23:39.733206Z","steps":["trace[1087901862] 'process raft request'  (duration: 120.091021ms)","trace[1087901862] 'compare'  (duration: 126.90641ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:40.182128Z","caller":"traceutil/trace.go:171","msg":"trace[1778441549] transaction","detail":"{read_only:false; response_revision:1929; number_of_response:1; }","duration":"138.169985ms","start":"2024-11-06T17:23:40.043907Z","end":"2024-11-06T17:23:40.182077Z","steps":["trace[1778441549] 'process raft request'  (duration: 137.559604ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:40.414372Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"163.898217ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033066449782806 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/endpointslices/kube-system/kube-dns-ms4ml\" mod_revision:1854 > success:<request_put:<key:\"/registry/endpointslices/kube-system/kube-dns-ms4ml\" value_size:1034 >> failure:<request_range:<key:\"/registry/endpointslices/kube-system/kube-dns-ms4ml\" > >>","response":"size:16"}
{"level":"info","ts":"2024-11-06T17:23:40.414812Z","caller":"traceutil/trace.go:171","msg":"trace[609883053] transaction","detail":"{read_only:false; response_revision:1931; number_of_response:1; }","duration":"220.228795ms","start":"2024-11-06T17:23:40.194540Z","end":"2024-11-06T17:23:40.414769Z","steps":["trace[609883053] 'process raft request'  (duration: 220.029787ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:40.414841Z","caller":"traceutil/trace.go:171","msg":"trace[960968546] transaction","detail":"{read_only:false; response_revision:1930; number_of_response:1; }","duration":"220.612084ms","start":"2024-11-06T17:23:40.194171Z","end":"2024-11-06T17:23:40.414783Z","steps":["trace[960968546] 'process raft request'  (duration: 56.157093ms)","trace[960968546] 'compare'  (duration: 163.690755ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:40.472091Z","caller":"traceutil/trace.go:171","msg":"trace[191237085] transaction","detail":"{read_only:false; response_revision:1932; number_of_response:1; }","duration":"198.597254ms","start":"2024-11-06T17:23:40.273446Z","end":"2024-11-06T17:23:40.472043Z","steps":["trace[191237085] 'process raft request'  (duration: 198.1943ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:40.472297Z","caller":"traceutil/trace.go:171","msg":"trace[1540732344] transaction","detail":"{read_only:false; response_revision:1933; number_of_response:1; }","duration":"172.672331ms","start":"2024-11-06T17:23:40.299584Z","end":"2024-11-06T17:23:40.472256Z","steps":["trace[1540732344] 'process raft request'  (duration: 172.344533ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:40.639812Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.566953ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-11-06T17:23:40.639957Z","caller":"traceutil/trace.go:171","msg":"trace[800858492] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:1933; }","duration":"105.725605ms","start":"2024-11-06T17:23:40.534195Z","end":"2024-11-06T17:23:40.639921Z","steps":["trace[800858492] 'range keys from in-memory index tree'  (duration: 105.183248ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:40.861390Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.063692ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033066449782814 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1891 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128033066449782812 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-11-06T17:23:40.861692Z","caller":"traceutil/trace.go:171","msg":"trace[460920984] transaction","detail":"{read_only:false; response_revision:1934; number_of_response:1; }","duration":"177.1257ms","start":"2024-11-06T17:23:40.684427Z","end":"2024-11-06T17:23:40.861553Z","steps":["trace[460920984] 'process raft request'  (duration: 63.78348ms)","trace[460920984] 'compare'  (duration: 112.732079ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:42.229244Z","caller":"traceutil/trace.go:171","msg":"trace[273649070] transaction","detail":"{read_only:false; response_revision:1936; number_of_response:1; }","duration":"155.125901ms","start":"2024-11-06T17:23:42.074071Z","end":"2024-11-06T17:23:42.229197Z","steps":["trace[273649070] 'process raft request'  (duration: 154.875747ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:46.414758Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"143.883034ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033066449782839 > lease_revoke:<id:70cc9302805e47cd>","response":"size:29"}
{"level":"info","ts":"2024-11-06T17:23:46.415105Z","caller":"traceutil/trace.go:171","msg":"trace[31330592] linearizableReadLoop","detail":"{readStateIndex:2318; appliedIndex:2317; }","duration":"248.944963ms","start":"2024-11-06T17:23:46.166116Z","end":"2024-11-06T17:23:46.415061Z","steps":["trace[31330592] 'read index received'  (duration: 104.760917ms)","trace[31330592] 'applied index is now lower than readState.Index'  (duration: 144.179638ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:46.415214Z","caller":"traceutil/trace.go:171","msg":"trace[1273170113] transaction","detail":"{read_only:false; response_revision:1940; number_of_response:1; }","duration":"243.74595ms","start":"2024-11-06T17:23:46.171434Z","end":"2024-11-06T17:23:46.415180Z","steps":["trace[1273170113] 'process raft request'  (duration: 243.509467ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:46.415412Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"249.266851ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/task-message-8576c8bf94-nwdj8\" ","response":"range_response_count:1 size:2927"}
{"level":"info","ts":"2024-11-06T17:23:46.415514Z","caller":"traceutil/trace.go:171","msg":"trace[1273794622] range","detail":"{range_begin:/registry/pods/default/task-message-8576c8bf94-nwdj8; range_end:; response_count:1; response_revision:1940; }","duration":"249.383202ms","start":"2024-11-06T17:23:46.166103Z","end":"2024-11-06T17:23:46.415487Z","steps":["trace[1273794622] 'agreement among raft nodes before linearized reading'  (duration: 249.120964ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:46.748203Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.189497ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:46.748297Z","caller":"traceutil/trace.go:171","msg":"trace[259398119] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1941; }","duration":"119.295829ms","start":"2024-11-06T17:23:46.628976Z","end":"2024-11-06T17:23:46.748272Z","steps":["trace[259398119] 'range keys from in-memory index tree'  (duration: 119.168491ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:23:46.748336Z","caller":"traceutil/trace.go:171","msg":"trace[883824761] transaction","detail":"{read_only:false; response_revision:1942; number_of_response:1; }","duration":"318.361412ms","start":"2024-11-06T17:23:46.429939Z","end":"2024-11-06T17:23:46.748300Z","steps":["trace[883824761] 'process raft request'  (duration: 265.799097ms)","trace[883824761] 'compare'  (duration: 52.38026ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T17:23:46.748467Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-11-06T17:23:46.429902Z","time spent":"318.482611ms","remote":"127.0.0.1:43784","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3191,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/task-message-8576c8bf94-nwdj8\" mod_revision:1865 > success:<request_put:<key:\"/registry/pods/default/task-message-8576c8bf94-nwdj8\" value_size:3131 >> failure:<request_range:<key:\"/registry/pods/default/task-message-8576c8bf94-nwdj8\" > >"}
{"level":"info","ts":"2024-11-06T17:23:50.674855Z","caller":"traceutil/trace.go:171","msg":"trace[1535030253] transaction","detail":"{read_only:false; response_revision:1946; number_of_response:1; }","duration":"122.944454ms","start":"2024-11-06T17:23:50.551865Z","end":"2024-11-06T17:23:50.674810Z","steps":["trace[1535030253] 'process raft request'  (duration: 60.577632ms)","trace[1535030253] 'compare'  (duration: 61.987213ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:23:50.956895Z","caller":"traceutil/trace.go:171","msg":"trace[480228604] transaction","detail":"{read_only:false; response_revision:1948; number_of_response:1; }","duration":"135.91109ms","start":"2024-11-06T17:23:50.820914Z","end":"2024-11-06T17:23:50.956825Z","steps":["trace[480228604] 'process raft request'  (duration: 135.482423ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T17:23:51.260600Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.990442ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T17:23:51.260793Z","caller":"traceutil/trace.go:171","msg":"trace[353676026] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1948; }","duration":"118.190578ms","start":"2024-11-06T17:23:51.142569Z","end":"2024-11-06T17:23:51.260760Z","steps":["trace[353676026] 'range keys from in-memory index tree'  (duration: 117.758366ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T17:24:15.959746Z","caller":"traceutil/trace.go:171","msg":"trace[1268025181] transaction","detail":"{read_only:false; response_revision:1966; number_of_response:1; }","duration":"141.706933ms","start":"2024-11-06T17:24:15.817943Z","end":"2024-11-06T17:24:15.959649Z","steps":["trace[1268025181] 'process raft request'  (duration: 87.247616ms)","trace[1268025181] 'compare'  (duration: 54.220006ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:24:23.660082Z","caller":"traceutil/trace.go:171","msg":"trace[703566285] transaction","detail":"{read_only:false; response_revision:1972; number_of_response:1; }","duration":"104.271849ms","start":"2024-11-06T17:24:23.555778Z","end":"2024-11-06T17:24:23.660050Z","steps":["trace[703566285] 'process raft request'  (duration: 86.005917ms)","trace[703566285] 'compare'  (duration: 17.095745ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T17:24:23.660591Z","caller":"traceutil/trace.go:171","msg":"trace[901500718] transaction","detail":"{read_only:false; response_revision:1973; number_of_response:1; }","duration":"104.23614ms","start":"2024-11-06T17:24:23.556336Z","end":"2024-11-06T17:24:23.660573Z","steps":["trace[901500718] 'process raft request'  (duration: 103.891659ms)"],"step_count":1}


==> etcd [9d1a229c7fb2] <==
{"level":"info","ts":"2024-11-06T15:48:16.418498Z","caller":"traceutil/trace.go:171","msg":"trace[591741766] transaction","detail":"{read_only:false; response_revision:820; number_of_response:1; }","duration":"113.070439ms","start":"2024-11-06T15:48:16.305336Z","end":"2024-11-06T15:48:16.418407Z","steps":["trace[591741766] 'process raft request'  (duration: 112.864681ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:48:16.418623Z","caller":"traceutil/trace.go:171","msg":"trace[704133129] transaction","detail":"{read_only:false; response_revision:819; number_of_response:1; }","duration":"113.558128ms","start":"2024-11-06T15:48:16.305020Z","end":"2024-11-06T15:48:16.418578Z","steps":["trace[704133129] 'process raft request'  (duration: 52.406802ms)","trace[704133129] 'compare'  (duration: 60.412499ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T15:48:29.072289Z","caller":"traceutil/trace.go:171","msg":"trace[234396355] transaction","detail":"{read_only:false; response_revision:838; number_of_response:1; }","duration":"107.977177ms","start":"2024-11-06T15:48:28.964260Z","end":"2024-11-06T15:48:29.072238Z","steps":["trace[234396355] 'process raft request'  (duration: 90.004306ms)","trace[234396355] 'compare'  (duration: 17.679305ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T15:49:25.587753Z","caller":"traceutil/trace.go:171","msg":"trace[1708932136] transaction","detail":"{read_only:false; response_revision:893; number_of_response:1; }","duration":"101.398278ms","start":"2024-11-06T15:49:25.486272Z","end":"2024-11-06T15:49:25.587671Z","steps":["trace[1708932136] 'process raft request'  (duration: 52.726779ms)","trace[1708932136] 'compare'  (duration: 48.263927ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T15:49:56.637038Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.390324ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033064865083142 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:922 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-11-06T15:49:56.637255Z","caller":"traceutil/trace.go:171","msg":"trace[1385334216] transaction","detail":"{read_only:false; response_revision:924; number_of_response:1; }","duration":"183.279464ms","start":"2024-11-06T15:49:56.453930Z","end":"2024-11-06T15:49:56.637210Z","steps":["trace[1385334216] 'process raft request'  (duration: 65.586405ms)","trace[1385334216] 'compare'  (duration: 117.17352ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T15:49:57.544056Z","caller":"traceutil/trace.go:171","msg":"trace[977950786] transaction","detail":"{read_only:false; response_revision:926; number_of_response:1; }","duration":"112.288828ms","start":"2024-11-06T15:49:57.431707Z","end":"2024-11-06T15:49:57.543996Z","steps":["trace[977950786] 'process raft request'  (duration: 68.196414ms)","trace[977950786] 'compare'  (duration: 43.838827ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T15:49:58.617064Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":648}
{"level":"info","ts":"2024-11-06T15:49:58.750279Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":648,"took":"132.468106ms","hash":281390927,"current-db-size-bytes":2097152,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":2097152,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-11-06T15:49:58.750420Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":281390927,"revision":648,"compact-revision":-1}
{"level":"info","ts":"2024-11-06T15:49:58.750868Z","caller":"traceutil/trace.go:171","msg":"trace[2113820427] transaction","detail":"{read_only:false; response_revision:930; number_of_response:1; }","duration":"100.160908ms","start":"2024-11-06T15:49:58.650646Z","end":"2024-11-06T15:49:58.750807Z","steps":["trace[2113820427] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 91.334778ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:50:59.593920Z","caller":"traceutil/trace.go:171","msg":"trace[718905065] transaction","detail":"{read_only:false; response_revision:987; number_of_response:1; }","duration":"129.988431ms","start":"2024-11-06T15:50:59.463896Z","end":"2024-11-06T15:50:59.593884Z","steps":["trace[718905065] 'process raft request'  (duration: 129.729949ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:51:27.430038Z","caller":"traceutil/trace.go:171","msg":"trace[1217346287] transaction","detail":"{read_only:false; response_revision:1009; number_of_response:1; }","duration":"473.738349ms","start":"2024-11-06T15:51:26.956244Z","end":"2024-11-06T15:51:27.429982Z","steps":["trace[1217346287] 'process raft request'  (duration: 473.361738ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T15:51:27.430326Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-11-06T15:51:26.956220Z","time spent":"473.960447ms","remote":"127.0.0.1:40268","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1008 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-11-06T15:52:51.760742Z","caller":"traceutil/trace.go:171","msg":"trace[1991210469] transaction","detail":"{read_only:false; response_revision:1078; number_of_response:1; }","duration":"113.1424ms","start":"2024-11-06T15:52:51.647550Z","end":"2024-11-06T15:52:51.760693Z","steps":["trace[1991210469] 'process raft request'  (duration: 112.863374ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:53:21.087779Z","caller":"traceutil/trace.go:171","msg":"trace[1285497119] linearizableReadLoop","detail":"{readStateIndex:1278; appliedIndex:1277; }","duration":"102.266788ms","start":"2024-11-06T15:53:20.985466Z","end":"2024-11-06T15:53:21.087733Z","steps":["trace[1285497119] 'read index received'  (duration: 101.9147ms)","trace[1285497119] 'applied index is now lower than readState.Index'  (duration: 350.299¬µs)"],"step_count":2}
{"level":"info","ts":"2024-11-06T15:53:21.087832Z","caller":"traceutil/trace.go:171","msg":"trace[1595026041] transaction","detail":"{read_only:false; response_revision:1101; number_of_response:1; }","duration":"274.518763ms","start":"2024-11-06T15:53:20.813264Z","end":"2024-11-06T15:53:21.087783Z","steps":["trace[1595026041] 'process raft request'  (duration: 274.280478ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-06T15:53:21.088020Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.560771ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-11-06T15:53:21.088125Z","caller":"traceutil/trace.go:171","msg":"trace[1533834241] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1101; }","duration":"102.679975ms","start":"2024-11-06T15:53:20.985416Z","end":"2024-11-06T15:53:21.088096Z","steps":["trace[1533834241] 'agreement among raft nodes before linearized reading'  (duration: 102.503584ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:53:47.194025Z","caller":"traceutil/trace.go:171","msg":"trace[1824649074] transaction","detail":"{read_only:false; response_revision:1122; number_of_response:1; }","duration":"141.138432ms","start":"2024-11-06T15:53:47.052838Z","end":"2024-11-06T15:53:47.193977Z","steps":["trace[1824649074] 'process raft request'  (duration: 140.909108ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:54:58.678887Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":929}
{"level":"info","ts":"2024-11-06T15:54:58.767426Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":929,"took":"88.147932ms","hash":1061908438,"current-db-size-bytes":2097152,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-11-06T15:54:58.767532Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1061908438,"revision":929,"compact-revision":648}
{"level":"info","ts":"2024-11-06T15:56:02.626782Z","caller":"traceutil/trace.go:171","msg":"trace[1029338952] transaction","detail":"{read_only:false; response_revision:1232; number_of_response:1; }","duration":"114.993591ms","start":"2024-11-06T15:56:02.511747Z","end":"2024-11-06T15:56:02.626740Z","steps":["trace[1029338952] 'process raft request'  (duration: 114.784703ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:56:25.228969Z","caller":"traceutil/trace.go:171","msg":"trace[956734394] transaction","detail":"{read_only:false; response_revision:1249; number_of_response:1; }","duration":"132.680172ms","start":"2024-11-06T15:56:25.096243Z","end":"2024-11-06T15:56:25.228924Z","steps":["trace[956734394] 'process raft request'  (duration: 132.489871ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:56:35.558203Z","caller":"traceutil/trace.go:171","msg":"trace[1650377781] transaction","detail":"{read_only:false; response_revision:1257; number_of_response:1; }","duration":"132.175316ms","start":"2024-11-06T15:56:35.425980Z","end":"2024-11-06T15:56:35.558155Z","steps":["trace[1650377781] 'process raft request'  (duration: 131.988766ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:58:21.646695Z","caller":"traceutil/trace.go:171","msg":"trace[578479271] transaction","detail":"{read_only:false; response_revision:1343; number_of_response:1; }","duration":"111.479888ms","start":"2024-11-06T15:58:21.535161Z","end":"2024-11-06T15:58:21.646641Z","steps":["trace[578479271] 'process raft request'  (duration: 72.605914ms)","trace[578479271] 'compare'  (duration: 38.630065ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T15:58:59.351278Z","caller":"traceutil/trace.go:171","msg":"trace[378763856] transaction","detail":"{read_only:false; response_revision:1372; number_of_response:1; }","duration":"125.255166ms","start":"2024-11-06T15:58:59.226001Z","end":"2024-11-06T15:58:59.351256Z","steps":["trace[378763856] 'process raft request'  (duration: 125.128508ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T15:59:58.728941Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1178}
{"level":"info","ts":"2024-11-06T15:59:58.884111Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1178,"took":"154.601162ms","hash":3680270836,"current-db-size-bytes":2097152,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1515520,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-11-06T15:59:58.884240Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3680270836,"revision":1178,"compact-revision":929}
{"level":"info","ts":"2024-11-06T16:00:18.887772Z","caller":"traceutil/trace.go:171","msg":"trace[1225179512] transaction","detail":"{read_only:false; response_revision:1436; number_of_response:1; }","duration":"118.237714ms","start":"2024-11-06T16:00:18.769498Z","end":"2024-11-06T16:00:18.887736Z","steps":["trace[1225179512] 'process raft request'  (duration: 118.017632ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:02:26.311979Z","caller":"traceutil/trace.go:171","msg":"trace[699865906] transaction","detail":"{read_only:false; response_revision:1538; number_of_response:1; }","duration":"145.988425ms","start":"2024-11-06T16:02:26.165942Z","end":"2024-11-06T16:02:26.311930Z","steps":["trace[699865906] 'process raft request'  (duration: 145.689656ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:02:57.459479Z","caller":"traceutil/trace.go:171","msg":"trace[629800328] transaction","detail":"{read_only:false; response_revision:1563; number_of_response:1; }","duration":"188.644279ms","start":"2024-11-06T16:02:57.270750Z","end":"2024-11-06T16:02:57.459395Z","steps":["trace[629800328] 'process raft request'  (duration: 188.01427ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:03:15.848137Z","caller":"traceutil/trace.go:171","msg":"trace[143702635] linearizableReadLoop","detail":"{readStateIndex:1876; appliedIndex:1875; }","duration":"147.92595ms","start":"2024-11-06T16:03:15.700162Z","end":"2024-11-06T16:03:15.848088Z","steps":["trace[143702635] 'read index received'  (duration: 33.38259ms)","trace[143702635] 'applied index is now lower than readState.Index'  (duration: 114.541317ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T16:03:15.848419Z","caller":"traceutil/trace.go:171","msg":"trace[880882239] transaction","detail":"{read_only:false; response_revision:1577; number_of_response:1; }","duration":"149.026437ms","start":"2024-11-06T16:03:15.699369Z","end":"2024-11-06T16:03:15.848396Z","steps":["trace[880882239] 'process raft request'  (duration: 111.644202ms)","trace[880882239] 'compare'  (duration: 36.935677ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T16:03:38.397685Z","caller":"traceutil/trace.go:171","msg":"trace[752178383] transaction","detail":"{read_only:false; response_revision:1596; number_of_response:1; }","duration":"139.043034ms","start":"2024-11-06T16:03:38.258597Z","end":"2024-11-06T16:03:38.397640Z","steps":["trace[752178383] 'process raft request'  (duration: 65.550441ms)","trace[752178383] 'compare'  (duration: 73.330057ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T16:03:48.556397Z","caller":"traceutil/trace.go:171","msg":"trace[2129447585] transaction","detail":"{read_only:false; response_revision:1603; number_of_response:1; }","duration":"105.516402ms","start":"2024-11-06T16:03:48.450835Z","end":"2024-11-06T16:03:48.556351Z","steps":["trace[2129447585] 'process raft request'  (duration: 105.233737ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:04:25.499606Z","caller":"traceutil/trace.go:171","msg":"trace[619993006] transaction","detail":"{read_only:false; response_revision:1632; number_of_response:1; }","duration":"104.204ms","start":"2024-11-06T16:04:25.395348Z","end":"2024-11-06T16:04:25.499552Z","steps":["trace[619993006] 'process raft request'  (duration: 103.905543ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:04:58.825809Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1419}
{"level":"info","ts":"2024-11-06T16:04:58.980840Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1419,"took":"154.222106ms","hash":1653612250,"current-db-size-bytes":2097152,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1511424,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-11-06T16:04:58.980982Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1653612250,"revision":1419,"compact-revision":1178}
{"level":"info","ts":"2024-11-06T16:05:59.968215Z","caller":"traceutil/trace.go:171","msg":"trace[1336117484] transaction","detail":"{read_only:false; response_revision:1709; number_of_response:1; }","duration":"115.464254ms","start":"2024-11-06T16:05:59.852724Z","end":"2024-11-06T16:05:59.968188Z","steps":["trace[1336117484] 'process raft request'  (duration: 115.323642ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:06:20.647905Z","caller":"traceutil/trace.go:171","msg":"trace[1148064112] linearizableReadLoop","detail":"{readStateIndex:2065; appliedIndex:2064; }","duration":"111.056971ms","start":"2024-11-06T16:06:20.536815Z","end":"2024-11-06T16:06:20.647872Z","steps":["trace[1148064112] 'read index received'  (duration: 88.002979ms)","trace[1148064112] 'applied index is now lower than readState.Index'  (duration: 23.052729ms)"],"step_count":2}
{"level":"info","ts":"2024-11-06T16:06:20.647987Z","caller":"traceutil/trace.go:171","msg":"trace[939818933] transaction","detail":"{read_only:false; response_revision:1726; number_of_response:1; }","duration":"112.95435ms","start":"2024-11-06T16:06:20.534976Z","end":"2024-11-06T16:06:20.647930Z","steps":["trace[939818933] 'process raft request'  (duration: 89.851021ms)","trace[939818933] 'compare'  (duration: 22.874647ms)"],"step_count":2}
{"level":"warn","ts":"2024-11-06T16:06:20.648165Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.302808ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2024-11-06T16:06:20.648328Z","caller":"traceutil/trace.go:171","msg":"trace[626991355] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:1726; }","duration":"111.494106ms","start":"2024-11-06T16:06:20.536793Z","end":"2024-11-06T16:06:20.648287Z","steps":["trace[626991355] 'agreement among raft nodes before linearized reading'  (duration: 111.229817ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:08:03.078970Z","caller":"traceutil/trace.go:171","msg":"trace[1272580351] transaction","detail":"{read_only:false; response_revision:1806; number_of_response:1; }","duration":"148.110518ms","start":"2024-11-06T16:08:02.930808Z","end":"2024-11-06T16:08:03.078919Z","steps":["trace[1272580351] 'process raft request'  (duration: 147.856988ms)"],"step_count":1}
{"level":"info","ts":"2024-11-06T16:08:42.479929Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-11-06T16:08:42.865564Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-11-06T16:08:43.255905Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-06T16:08:43.544802Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-06T16:08:43.544929Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-06T16:08:43.681407Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-06T16:08:44.026914Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2024-11-06T16:08:44.027610Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"info","ts":"2024-11-06T16:08:45.027610Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-11-06T16:08:45.850121Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-06T16:08:45.876016Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-06T16:08:45.876321Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 17:25:29 up 8 min,  0 users,  load average: 1.25, 2.65, 1.68
Linux minikube 6.8.0-45-generic #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Sep 11 15:25:05 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [221506fed780] <==
W1106 17:23:18.239550       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1106 17:23:18.239559       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1106 17:23:18.240571       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1106 17:23:18.240589       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1106 17:23:18.261140       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1106 17:23:18.261266       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1106 17:23:19.111836       1 secure_serving.go:213] Serving securely on [::]:8443
I1106 17:23:19.112132       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1106 17:23:19.112434       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1106 17:23:19.112731       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1106 17:23:19.116282       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1106 17:23:19.116380       1 controller.go:78] Starting OpenAPI AggregationController
I1106 17:23:19.127260       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1106 17:23:19.125740       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1106 17:23:19.126109       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1106 17:23:19.127468       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1106 17:23:19.162168       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1106 17:23:19.131069       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1106 17:23:19.140508       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1106 17:23:19.140745       1 local_available_controller.go:156] Starting LocalAvailability controller
I1106 17:23:19.162844       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1106 17:23:19.141004       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1106 17:23:19.162932       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1106 17:23:19.141030       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1106 17:23:19.162980       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1106 17:23:19.141208       1 controller.go:119] Starting legacy_token_tracking_controller
I1106 17:23:19.230703       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1106 17:23:19.141249       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1106 17:23:19.141796       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1106 17:23:19.231959       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1106 17:23:19.149769       1 aggregator.go:169] waiting for initial CRD sync...
I1106 17:23:19.260949       1 controller.go:142] Starting OpenAPI controller
I1106 17:23:19.261023       1 controller.go:90] Starting OpenAPI V3 controller
I1106 17:23:19.261054       1 naming_controller.go:294] Starting NamingConditionController
I1106 17:23:19.261103       1 establishing_controller.go:81] Starting EstablishingController
I1106 17:23:19.261147       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1106 17:23:19.261173       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1106 17:23:19.261198       1 crd_finalizer.go:269] Starting CRDFinalizer
I1106 17:23:19.261259       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1106 17:23:19.261413       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1106 17:23:19.331092       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1106 17:23:19.331120       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1106 17:23:19.331283       1 shared_informer.go:320] Caches are synced for configmaps
I1106 17:23:19.332248       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1106 17:23:19.332290       1 aggregator.go:171] initial CRD sync complete...
I1106 17:23:19.332323       1 autoregister_controller.go:144] Starting autoregister controller
I1106 17:23:19.332337       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1106 17:23:19.332353       1 cache.go:39] Caches are synced for autoregister controller
I1106 17:23:19.339561       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1106 17:23:19.339698       1 policy_source.go:224] refreshing policies
I1106 17:23:19.341797       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1106 17:23:19.351917       1 shared_informer.go:320] Caches are synced for node_authorizer
I1106 17:23:19.362533       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1106 17:23:19.363029       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1106 17:23:19.363188       1 cache.go:39] Caches are synced for LocalAvailability controller
I1106 17:23:19.363433       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1106 17:23:19.371888       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1106 17:23:20.138043       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1106 17:23:23.518683       1 controller.go:615] quota admission added evaluator for: endpoints
I1106 17:23:23.647305       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-apiserver [ed41c10b6569] <==
W1106 16:08:47.100742       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.596709       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.753292       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.793897       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.838287       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.897698       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.906425       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.911810       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.920771       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.969182       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:48.992580       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.002886       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.025292       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.029991       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.039192       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.059402       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.064175       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.114197       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.118057       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.142509       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.150846       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.170275       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.178248       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.189163       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.199546       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.210918       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.220038       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.225732       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.285754       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.288595       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.320618       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.347508       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.357814       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.373007       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.391986       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.394759       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.403406       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.423031       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.449350       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.460307       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.479565       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.494735       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.538203       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.541106       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.541108       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.587886       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.606393       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.606488       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.606408       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.666211       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.711346       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.741248       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.744271       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.779227       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.798254       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:49.988511       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:50.001400       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:50.059540       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:50.076216       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1106 16:08:50.156482       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [1c59ab2508c1] <==
I1106 15:40:16.791072       1 shared_informer.go:320] Caches are synced for deployment
I1106 15:40:16.792395       1 shared_informer.go:320] Caches are synced for taint
I1106 15:40:16.792690       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1106 15:40:16.792906       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1106 15:40:16.793070       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1106 15:40:16.805764       1 shared_informer.go:320] Caches are synced for job
I1106 15:40:16.805978       1 shared_informer.go:320] Caches are synced for cronjob
I1106 15:40:16.818636       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1106 15:40:16.818704       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:40:16.819255       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:40:16.837358       1 shared_informer.go:320] Caches are synced for resource quota
I1106 15:40:16.866264       1 shared_informer.go:320] Caches are synced for attach detach
I1106 15:40:16.883797       1 shared_informer.go:320] Caches are synced for resource quota
I1106 15:40:17.258828       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:40:17.267452       1 shared_informer.go:320] Caches are synced for garbage collector
I1106 15:40:17.269779       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1106 15:40:17.297264       1 shared_informer.go:320] Caches are synced for garbage collector
I1106 15:40:17.983250       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:40:19.253744       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="731.879469ms"
I1106 15:40:19.592914       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="336.93432ms"
I1106 15:40:19.593199       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="175.877¬µs"
I1106 15:40:20.294379       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="2.193949ms"
I1106 15:40:20.677279       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="211.562¬µs"
I1106 15:40:23.518123       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:40:26.289417       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:40:32.283304       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="1.112040911s"
I1106 15:40:32.903218       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="619.787241ms"
I1106 15:40:32.904031       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="179.541¬µs"
I1106 15:40:36.699973       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="185.442¬µs"
I1106 15:40:41.884551       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="161.931¬µs"
I1106 15:40:47.000130       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="206.875¬µs"
I1106 15:40:48.117167       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="196.309¬µs"
I1106 15:40:48.443864       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="165.113¬µs"
I1106 15:41:18.187294       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="394.13712ms"
I1106 15:41:18.187528       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="148.932¬µs"
I1106 15:45:28.988946       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:48:16.344934       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="255.325801ms"
I1106 15:48:16.429668       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="84.419705ms"
I1106 15:48:16.429842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="82.651¬µs"
I1106 15:48:16.448344       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="73.979¬µs"
I1106 15:48:29.079018       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="121.547¬µs"
I1106 15:48:44.522081       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="65.835¬µs"
I1106 15:49:14.526529       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="152.535¬µs"
I1106 15:49:25.529931       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="209.434¬µs"
I1106 15:49:43.517110       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="122.022¬µs"
I1106 15:49:57.549935       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="1.592254ms"
I1106 15:50:34.865046       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 15:50:36.493386       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="135.598¬µs"
I1106 15:50:49.509793       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="92.573¬µs"
I1106 15:52:09.492456       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="223.835¬µs"
I1106 15:52:22.470270       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="217.141¬µs"
I1106 15:55:09.474741       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="183.419¬µs"
I1106 15:55:21.466712       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="215.633¬µs"
I1106 15:55:40.641268       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 16:00:14.519028       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="194.122¬µs"
I1106 16:00:26.507631       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="346.474¬µs"
I1106 16:00:47.013348       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 16:05:25.445944       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="268.336¬µs"
I1106 16:05:38.495856       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="115.811¬µs"
I1106 16:05:53.650981       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [cacd0f5b2b29] <==
I1106 17:23:22.576950       1 shared_informer.go:320] Caches are synced for cronjob
I1106 17:23:22.578615       1 shared_informer.go:320] Caches are synced for job
I1106 17:23:22.579451       1 shared_informer.go:320] Caches are synced for PV protection
I1106 17:23:22.579623       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1106 17:23:22.579812       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1106 17:23:22.586761       1 shared_informer.go:320] Caches are synced for ephemeral
I1106 17:23:22.594172       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1106 17:23:22.598422       1 shared_informer.go:320] Caches are synced for service account
I1106 17:23:22.606738       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1106 17:23:22.612913       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1106 17:23:22.614162       1 shared_informer.go:320] Caches are synced for TTL
I1106 17:23:22.614219       1 shared_informer.go:320] Caches are synced for stateful set
I1106 17:23:22.616696       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1106 17:23:22.618929       1 shared_informer.go:320] Caches are synced for HPA
I1106 17:23:22.632252       1 shared_informer.go:320] Caches are synced for namespace
I1106 17:23:22.632374       1 shared_informer.go:320] Caches are synced for persistent volume
I1106 17:23:22.637191       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1106 17:23:22.637400       1 shared_informer.go:320] Caches are synced for daemon sets
I1106 17:23:22.666587       1 shared_informer.go:320] Caches are synced for ReplicationController
I1106 17:23:22.666739       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="29.4161ms"
I1106 17:23:22.666891       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1106 17:23:22.666912       1 shared_informer.go:320] Caches are synced for GC
I1106 17:23:22.666739       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="29.44343ms"
I1106 17:23:22.666595       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1106 17:23:22.666618       1 shared_informer.go:320] Caches are synced for endpoint
I1106 17:23:22.668771       1 shared_informer.go:320] Caches are synced for PVC protection
I1106 17:23:22.668823       1 shared_informer.go:320] Caches are synced for node
I1106 17:23:22.668896       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1106 17:23:22.668951       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1106 17:23:22.668966       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1106 17:23:22.668979       1 shared_informer.go:320] Caches are synced for cidrallocator
I1106 17:23:22.669154       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1106 17:23:22.669217       1 shared_informer.go:320] Caches are synced for deployment
I1106 17:23:22.672193       1 shared_informer.go:320] Caches are synced for disruption
I1106 17:23:22.672475       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1106 17:23:22.684405       1 shared_informer.go:320] Caches are synced for attach detach
I1106 17:23:22.692467       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1106 17:23:22.767398       1 shared_informer.go:320] Caches are synced for taint
I1106 17:23:22.767723       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1106 17:23:22.767904       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1106 17:23:22.768018       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1106 17:23:22.819017       1 shared_informer.go:320] Caches are synced for resource quota
I1106 17:23:22.822348       1 shared_informer.go:320] Caches are synced for resource quota
I1106 17:23:23.293418       1 shared_informer.go:320] Caches are synced for garbage collector
I1106 17:23:23.309901       1 shared_informer.go:320] Caches are synced for garbage collector
I1106 17:23:23.309928       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1106 17:23:23.726143       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="279.945588ms"
I1106 17:23:23.728003       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="93.004¬µs"
I1106 17:23:23.919554       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="EndpointSlice informer cache is out of date"
I1106 17:23:26.574445       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="2.159774ms"
I1106 17:23:40.188315       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="263.836¬µs"
I1106 17:23:46.751481       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="69.489¬µs"
I1106 17:23:58.710250       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="86.104¬µs"
I1106 17:24:12.662227       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="133.136¬µs"
I1106 17:24:23.671798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="129.801893ms"
I1106 17:24:23.672625       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="180.768¬µs"
I1106 17:24:27.656109       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="116.687¬µs"
I1106 17:24:41.697097       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="146.982¬µs"
I1106 17:24:56.657137       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="126.848¬µs"
I1106 17:25:27.693589       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/task-message-8576c8bf94" duration="92.85¬µs"


==> kube-proxy [0f4c8dc4c091] <==
I1106 15:40:38.065703       1 server_linux.go:66] "Using iptables proxy"
I1106 15:40:40.989115       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1106 15:40:40.989289       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1106 15:40:42.277270       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1106 15:40:42.277386       1 server_linux.go:169] "Using iptables Proxier"
I1106 15:40:42.283400       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1106 15:40:42.284465       1 server.go:483] "Version info" version="v1.31.0"
I1106 15:40:42.284580       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1106 15:40:42.499827       1 config.go:104] "Starting endpoint slice config controller"
I1106 15:40:42.499937       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1106 15:40:42.499940       1 config.go:197] "Starting service config controller"
I1106 15:40:42.500005       1 shared_informer.go:313] Waiting for caches to sync for service config
I1106 15:40:42.500142       1 config.go:326] "Starting node config controller"
I1106 15:40:42.500167       1 shared_informer.go:313] Waiting for caches to sync for node config
I1106 15:40:42.601076       1 shared_informer.go:320] Caches are synced for node config
I1106 15:40:42.601128       1 shared_informer.go:320] Caches are synced for service config
I1106 15:40:42.601233       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [c7f546976bf7] <==
I1106 17:23:48.854261       1 server_linux.go:66] "Using iptables proxy"
I1106 17:23:49.473345       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1106 17:23:49.473557       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1106 17:23:49.737437       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1106 17:23:49.737527       1 server_linux.go:169] "Using iptables Proxier"
I1106 17:23:49.743846       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1106 17:23:49.821194       1 server.go:483] "Version info" version="v1.31.0"
I1106 17:23:49.821354       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1106 17:23:49.984568       1 config.go:197] "Starting service config controller"
I1106 17:23:49.984638       1 config.go:326] "Starting node config controller"
I1106 17:23:49.984661       1 shared_informer.go:313] Waiting for caches to sync for service config
I1106 17:23:49.984726       1 shared_informer.go:313] Waiting for caches to sync for node config
I1106 17:23:49.984569       1 config.go:104] "Starting endpoint slice config controller"
I1106 17:23:49.985014       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1106 17:23:50.085854       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1106 17:23:50.085970       1 shared_informer.go:320] Caches are synced for service config
I1106 17:23:50.085889       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [77eef5262a3d] <==
W1106 17:23:14.679364       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.679480       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.679741       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.679831       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.679880       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.679996       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.680255       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.680347       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.680462       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.680567       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.680601       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.680717       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.680814       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.680906       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.680982       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.681080       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I1106 17:23:14.681341       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W1106 17:23:14.682093       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.682168       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.682420       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.682508       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.682763       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.682836       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.700517       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.700646       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.700527       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.701009       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:14.700524       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1106 17:23:14.701275       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1106 17:23:19.233788       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1106 17:23:19.234118       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1106 17:23:19.234175       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1106 17:23:19.234204       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.233788       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1106 17:23:19.235767       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.234399       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1106 17:23:19.236191       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.234557       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1106 17:23:19.236852       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.234691       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1106 17:23:19.236993       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.234795       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1106 17:23:19.237089       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.234989       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1106 17:23:19.237177       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.235130       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1106 17:23:19.237236       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.235240       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1106 17:23:19.237298       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.235366       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1106 17:23:19.237378       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.235494       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1106 17:23:19.237449       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.236564       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1106 17:23:19.237511       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.236625       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1106 17:23:19.237567       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 17:23:19.244552       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1106 17:23:19.244866       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1106 17:23:21.278305       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [cdc5852872fb] <==
E1106 15:40:03.302180       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.457772       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1106 15:40:03.457980       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.463974       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:03.464088       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.534920       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1106 15:40:03.535023       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.537495       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:03.537568       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.602966       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1106 15:40:03.603017       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.610968       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1106 15:40:03.611028       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.617229       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1106 15:40:03.617326       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.636643       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:03.636739       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.657406       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1106 15:40:03.657524       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.681643       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1106 15:40:03.681727       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:03.837516       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1106 15:40:03.837603       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1106 15:40:04.951058       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1106 15:40:04.951165       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1106 15:40:05.086501       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1106 15:40:05.086629       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:05.167592       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1106 15:40:05.167687       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:05.374681       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:05.374805       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:05.565001       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1106 15:40:05.565100       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:05.629005       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1106 15:40:05.629109       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:05.693863       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1106 15:40:05.693940       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:05.875617       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1106 15:40:05.875712       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1106 15:40:06.117084       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1106 15:40:06.117201       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:06.117216       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1106 15:40:06.117286       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:06.129501       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:06.129613       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:06.314517       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1106 15:40:06.314646       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1106 15:40:06.673152       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1106 15:40:06.673251       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1106 15:40:06.827901       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1106 15:40:06.827979       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1106 15:40:06.910453       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1106 15:40:06.910591       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1106 15:40:10.008917       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1106 15:40:10.008989       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1106 15:40:16.850418       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1106 16:08:43.262762       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1106 16:08:43.262807       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1106 16:08:43.357364       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E1106 16:08:44.039813       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Nov 06 17:23:20 minikube kubelet[1037]: E1106 17:23:20.851369    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:20 minikube kubelet[1037]: E1106 17:23:20.952427    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.053574    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.153815    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.255129    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.355698    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.456342    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.557299    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.657701    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.758405    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.859204    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:21 minikube kubelet[1037]: E1106 17:23:21.959795    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.061642    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.163066    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.264009    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.364868    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.465838    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.566009    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.667183    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.768213    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:22 minikube kubelet[1037]: E1106 17:23:22.869090    1037 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 06 17:23:23 minikube kubelet[1037]: I1106 17:23:23.078810    1037 apiserver.go:52] "Watching apiserver"
Nov 06 17:23:23 minikube kubelet[1037]: I1106 17:23:23.112034    1037 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Nov 06 17:23:23 minikube kubelet[1037]: I1106 17:23:23.184514    1037 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/aff9f9af-b0a7-4d02-993d-749513a63c7e-tmp\") pod \"storage-provisioner\" (UID: \"aff9f9af-b0a7-4d02-993d-749513a63c7e\") " pod="kube-system/storage-provisioner"
Nov 06 17:23:23 minikube kubelet[1037]: I1106 17:23:23.184570    1037 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/0be96ac5-8886-421f-86b2-5c2314ac1ab4-lib-modules\") pod \"kube-proxy-jrz7p\" (UID: \"0be96ac5-8886-421f-86b2-5c2314ac1ab4\") " pod="kube-system/kube-proxy-jrz7p"
Nov 06 17:23:23 minikube kubelet[1037]: I1106 17:23:23.184649    1037 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/0be96ac5-8886-421f-86b2-5c2314ac1ab4-xtables-lock\") pod \"kube-proxy-jrz7p\" (UID: \"0be96ac5-8886-421f-86b2-5c2314ac1ab4\") " pod="kube-system/kube-proxy-jrz7p"
Nov 06 17:23:29 minikube kubelet[1037]: E1106 17:23:29.271462    1037 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-wwxp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(aff9f9af-b0a7-4d02-993d-749513a63c7e): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
Nov 06 17:23:29 minikube kubelet[1037]: E1106 17:23:29.272726    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID="aff9f9af-b0a7-4d02-993d-749513a63c7e"
Nov 06 17:23:29 minikube kubelet[1037]: I1106 17:23:29.703710    1037 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1e9c75f26c5e1aa910d3a3a4278bfcb4a57494dbcbfb39bf7366fed26873e5ab"
Nov 06 17:23:29 minikube kubelet[1037]: I1106 17:23:29.718311    1037 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="05f314e9f0ea1abe90dffad8b4e20357a208e2449bf577062136645698b1271d"
Nov 06 17:23:29 minikube kubelet[1037]: I1106 17:23:29.719890    1037 scope.go:117] "RemoveContainer" containerID="ed60c8130446086a5faa37724f1d35bbf62a4a88fc4cf92c9fffb9c5c4600dc7"
Nov 06 17:23:32 minikube kubelet[1037]: I1106 17:23:32.103507    1037 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a9319aa738e22e6314db50cb171b5fe339ecbf92b4912fc7b0c6ec8b03484dda"
Nov 06 17:23:32 minikube kubelet[1037]: I1106 17:23:32.646011    1037 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="06d4a50bb48a6dee1ff28f85966af3be25d5e66df7dea2ea4a99c29e9906f3bf"
Nov 06 17:23:46 minikube kubelet[1037]: E1106 17:23:46.002240    1037 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:23:46 minikube kubelet[1037]: E1106 17:23:46.002409    1037 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:23:46 minikube kubelet[1037]: E1106 17:23:46.002748    1037 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:task-message-container,Image:task-message-image:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvw2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod task-message-8576c8bf94-nwdj8_default(1b08617d-422d-41aa-a2dc-f03998924804): ErrImagePull: Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 06 17:23:46 minikube kubelet[1037]: E1106 17:23:46.004123    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ErrImagePull: \"Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:23:46 minikube kubelet[1037]: E1106 17:23:46.167304    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ImagePullBackOff: \"Back-off pulling image \\\"task-message-image:latest\\\"\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:24:01 minikube kubelet[1037]: E1106 17:24:01.935415    1037 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:24:01 minikube kubelet[1037]: E1106 17:24:01.935860    1037 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:24:01 minikube kubelet[1037]: E1106 17:24:01.936179    1037 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:task-message-container,Image:task-message-image:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvw2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod task-message-8576c8bf94-nwdj8_default(1b08617d-422d-41aa-a2dc-f03998924804): ErrImagePull: Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 06 17:24:01 minikube kubelet[1037]: E1106 17:24:01.937539    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ErrImagePull: \"Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:24:12 minikube kubelet[1037]: E1106 17:24:12.595998    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ImagePullBackOff: \"Back-off pulling image \\\"task-message-image:latest\\\"\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:24:15 minikube kubelet[1037]: I1106 17:24:15.796659    1037 scope.go:117] "RemoveContainer" containerID="ed60c8130446086a5faa37724f1d35bbf62a4a88fc4cf92c9fffb9c5c4600dc7"
Nov 06 17:24:15 minikube kubelet[1037]: I1106 17:24:15.797610    1037 scope.go:117] "RemoveContainer" containerID="4034c95d57681f98724b47bd8355a12f0dbafa74c2e788ffb527fcb975e3d8dd"
Nov 06 17:24:15 minikube kubelet[1037]: E1106 17:24:15.798250    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(aff9f9af-b0a7-4d02-993d-749513a63c7e)\"" pod="kube-system/storage-provisioner" podUID="aff9f9af-b0a7-4d02-993d-749513a63c7e"
Nov 06 17:24:29 minikube kubelet[1037]: I1106 17:24:29.589011    1037 scope.go:117] "RemoveContainer" containerID="4034c95d57681f98724b47bd8355a12f0dbafa74c2e788ffb527fcb975e3d8dd"
Nov 06 17:24:29 minikube kubelet[1037]: E1106 17:24:29.589591    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(aff9f9af-b0a7-4d02-993d-749513a63c7e)\"" pod="kube-system/storage-provisioner" podUID="aff9f9af-b0a7-4d02-993d-749513a63c7e"
Nov 06 17:24:29 minikube kubelet[1037]: E1106 17:24:29.951763    1037 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:24:29 minikube kubelet[1037]: E1106 17:24:29.951890    1037 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:24:29 minikube kubelet[1037]: E1106 17:24:29.952143    1037 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:task-message-container,Image:task-message-image:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvw2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod task-message-8576c8bf94-nwdj8_default(1b08617d-422d-41aa-a2dc-f03998924804): ErrImagePull: Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 06 17:24:29 minikube kubelet[1037]: E1106 17:24:29.953608    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ErrImagePull: \"Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:24:41 minikube kubelet[1037]: E1106 17:24:41.602460    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ImagePullBackOff: \"Back-off pulling image \\\"task-message-image:latest\\\"\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:24:42 minikube kubelet[1037]: I1106 17:24:42.587992    1037 scope.go:117] "RemoveContainer" containerID="4034c95d57681f98724b47bd8355a12f0dbafa74c2e788ffb527fcb975e3d8dd"
Nov 06 17:24:56 minikube kubelet[1037]: E1106 17:24:56.594131    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ImagePullBackOff: \"Back-off pulling image \\\"task-message-image:latest\\\"\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:25:12 minikube kubelet[1037]: E1106 17:25:12.860183    1037 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:25:12 minikube kubelet[1037]: E1106 17:25:12.860311    1037 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="task-message-image:latest"
Nov 06 17:25:12 minikube kubelet[1037]: E1106 17:25:12.860612    1037 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:task-message-container,Image:task-message-image:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvw2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod task-message-8576c8bf94-nwdj8_default(1b08617d-422d-41aa-a2dc-f03998924804): ErrImagePull: Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 06 17:25:12 minikube kubelet[1037]: E1106 17:25:12.862077    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ErrImagePull: \"Error response from daemon: pull access denied for task-message-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"
Nov 06 17:25:27 minikube kubelet[1037]: E1106 17:25:27.595162    1037 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-message-container\" with ImagePullBackOff: \"Back-off pulling image \\\"task-message-image:latest\\\"\"" pod="default/task-message-8576c8bf94-nwdj8" podUID="1b08617d-422d-41aa-a2dc-f03998924804"


==> storage-provisioner [32155398fa82] <==
I1106 17:24:43.797530       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1106 17:24:43.949817       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1106 17:24:43.949923       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1106 17:25:01.436564       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1106 17:25:01.436815       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d39f22ab-f55e-478b-bd9c-6571dafa1acf", APIVersion:"v1", ResourceVersion:"2001", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_bd5ee76d-a073-4ee5-b3c2-198c850250b7 became leader
I1106 17:25:01.436967       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_bd5ee76d-a073-4ee5-b3c2-198c850250b7!
I1106 17:25:01.538053       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_bd5ee76d-a073-4ee5-b3c2-198c850250b7!


==> storage-provisioner [4034c95d5768] <==
I1106 17:23:44.291687       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1106 17:24:14.573269       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

